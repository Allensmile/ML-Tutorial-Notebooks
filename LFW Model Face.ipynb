{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorw LFW RNN trainning\n",
    "To learn how to encode a simple image and a GIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import needed for Jupiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import needed for the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import fnmatch, os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A function to save a picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#need to be called within a session\n",
    "def write_png(tensor, name):\n",
    "    casted_to_uint8 = tf.cast(tensor, tf.uint8)\n",
    "    converted_to_png = tf.image.encode_png(casted_to_uint8)\n",
    "    f = open(name, \"wb+\")\n",
    "    f.write(converted_to_png.eval())\n",
    "    f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## A class to define all args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        '''directory to store checkpointed models'''\n",
    "        self.save_dir = 'save_face_training'\n",
    "        \n",
    "        '''Picture size'''\n",
    "        self.picture_size = 128\n",
    "    \n",
    "        '''size of RNN hidden state'''\n",
    "        self.rnn_size = 100 \n",
    "        '''minibatch size'''\n",
    "        self.batch_size = 1\n",
    "        '''RNN sequence length'''\n",
    "        self.seq_length = self.picture_size\n",
    "        '''number of epochs'''\n",
    "        self.num_epochs = 1 # was 5\n",
    "        '''save frequency'''\n",
    "        self.save_every = 50 # was 500\n",
    "        '''Print frequency'''\n",
    "        self.print_every = 1\n",
    "        '''clip gradients at this value'''\n",
    "        self.grad_clip = 5.\n",
    "        '''learning rate'''\n",
    "        self.learning_rate = 0.002 # was 0.002\n",
    "        '''decay rate for rmsprop'''\n",
    "        self.decay_rate = 0.98\n",
    "        \"\"\"continue training from saved model at this path.\n",
    "        Path must contain files saved by previous training process: \"\"\"\n",
    "        #self.init_from = 'save_face_training'\n",
    "        self.init_from = None\n",
    "        \n",
    "        '''number of ligne to sample'''\n",
    "        self.n = 250\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FaceLoader:\n",
    "    def prepare_reading_faces(self):\n",
    "        self.matches = []\n",
    "    \n",
    "        for root, dirnames, filenames in os.walk('./lfw/'):\n",
    "            #print filenames\n",
    "            for filename in fnmatch.filter(filenames, '*.jpg'):\n",
    "                self.matches.append(os.path.join(root, filename))\n",
    "\n",
    "        size = len(self.matches)\n",
    "\n",
    "        filenames = tf.constant(self.matches)\n",
    "        self.filename_queue = tf.train.string_input_producer(filenames)\n",
    "        self.image_reader = tf.WholeFileReader()\n",
    "        return size\n",
    "    \n",
    "    def do_when_session(self):   \n",
    "        # For some reason, we need a coordinator and some threads\n",
    "        self.coord = tf.train.Coordinator()\n",
    "        self.threads = tf.train.start_queue_runners(coord=self.coord)\n",
    "\n",
    "    def stop_reading_faces(self):\n",
    "        # Finish off the filename queue coordinator.\n",
    "        self.coord.request_stop()\n",
    "        self.coord.join(self.threads)\n",
    "          \n",
    "    def load_one_face(self, image_size):\n",
    "        # read and decode image, will give a uint8 with shape [250, 250, 1]\n",
    "        filename, image_file = self.image_reader.read(self.filename_queue)     \n",
    "        image = tf.image.decode_jpeg(image_file, channels=1)\n",
    "        #resize\n",
    "        image = tf.image.resize_images(image, image_size, image_size)\n",
    "\n",
    "        # remove channel dimension\n",
    "        tensor_uint8 = tf.squeeze(image, squeeze_dims=[2])\n",
    "\n",
    "        # convert to float32 and concat to all face\n",
    "        tensor = tf.cast(tensor_uint8, tf.float32)\n",
    "        a_vector_face = (tf.sign(tensor-128)+1)/2\n",
    "        #print a_vector_face.eval()[0][128]\n",
    "        a_bw_picture = a_vector_face*255\n",
    "        \n",
    "        #write_png(a_bw_picture, 'bw_face.png')\n",
    "        \n",
    "        # print some log\n",
    "        #print(filename.eval(),a_bw_picture.eval().shape)\n",
    "            \n",
    "        xdata = a_vector_face.eval()\n",
    "        ydata = np.copy(xdata)\n",
    "        ydata[:-1] = xdata[1:]\n",
    "        ydata[-1] = xdata[0]\n",
    "        self.x_batches = np.squeeze(np.split(xdata, image_size, 0))\n",
    "        self.y_batches = np.squeeze(np.split(ydata, image_size, 0))\n",
    "                \n",
    "    def next_batch(self):\n",
    "        return self.x_batches, self.y_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]] [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "faceloader = FaceLoader()\n",
    "face_count = faceloader.prepare_reading_faces()\n",
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    faceloader.do_when_session()\n",
    "    faceloader.load_one_face(64)\n",
    "    x, y = faceloader.next_batch();\n",
    "    print x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, args, infer=False):\n",
    "        self.args = args\n",
    "        if infer:\n",
    "            '''Infer is true when the model is used for sampling'''\n",
    "            args.seq_length = 1\n",
    "   \n",
    "        hidden_size = args.rnn_size\n",
    "        vector_size = args.picture_size\n",
    "        \n",
    "        # define place holder to for the input data and the target.\n",
    "        self.input_data = tf.placeholder(tf.float32, [ args.seq_length, vector_size], name='input_data')\n",
    "        self.target_data = tf.placeholder(tf.float32, [ args.seq_length, vector_size], name='target_data') \n",
    "        # define the input xs\n",
    "        xs = tf.split(0, args.seq_length, self.input_data)\n",
    "        # define the target\n",
    "        targets = tf.split(0, args.seq_length, self.target_data)  \n",
    "        #initial_state\n",
    "        self.initial_state = tf.zeros((hidden_size,1))\n",
    "        #last_state = tf.placeholder(tf.float32, (hidden_size, 1))\n",
    "        \n",
    "        # model parameters\n",
    "        Wxh = tf.Variable(tf.random_uniform((hidden_size, vector_size))*0.01, name='Wxh') # input to hidden\n",
    "        Whh = tf.Variable(tf.random_uniform((hidden_size, hidden_size))*0.01, name='Whh') # hidden to hidden\n",
    "        Why = tf.Variable(tf.random_uniform((vector_size, hidden_size))*0.01, name='Why') # hidden to output\n",
    "        bh = tf.Variable(tf.zeros((hidden_size, 1)), name='bh') # hidden bias\n",
    "        by = tf.Variable(tf.zeros((vector_size, 1)), name='by') # output bias\n",
    "        loss = tf.zeros([1], name='loss')\n",
    "        \n",
    "        hs, ys, ps = {}, {}, {}\n",
    "        \n",
    "        hs[-1] = self.initial_state\n",
    "        # forward pass                                                                                                                                                                              \n",
    "        for t in xrange(args.seq_length):\n",
    "            xs_t = tf.transpose(xs[t])\n",
    "            targets_t = tf.transpose(targets[t]) \n",
    "            \n",
    "            hs[t] = tf.tanh(tf.matmul(Wxh, xs_t) + tf.matmul(Whh, hs[t-1]) + bh) # hidden state\n",
    "            ys[t] = tf.matmul(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "            #ps[t] = tf.exp(ys[t]) / tf.reduce_max(tf.exp(ys[t])) # probabilities for next chars\n",
    "            #ps[t] = (tf.sign(ys[t]-0.5)+1)/2\n",
    "            #loss += -tf.log(tf.reduce_sum(tf.abs(ps[t]-targets_t))+0.00000001) # softmax (cross-entropy loss)\n",
    "            loss += tf.reduce_sum(tf.abs(ys[t]-targets_t))\n",
    "\n",
    "        self.probs = ys[args.seq_length-1]\n",
    "        self.cost = loss / args.batch_size / args.seq_length\n",
    "        self.final_state = hs[args.seq_length-1]\n",
    "        self.lr = tf.Variable(0.0, trainable=False, name='learning_rate')\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),\n",
    "                args.grad_clip)\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    def sample(self, sess):\n",
    "        size = self.args.picture_size\n",
    "        picture_vect = np.zeros((size, size))\n",
    "        state = model.initial_state.eval()\n",
    "        x = np.random.random([1, size])\n",
    "        feed = {self.input_data: x, self.initial_state:state}\n",
    "        [probs, state] = sess.run([self.probs, self.final_state], feed)\n",
    "        for n in range(size):\n",
    "            line = np.transpose(probs)\n",
    "            feed = {self.input_data: line, self.initial_state:state}\n",
    "            [probs, state] = sess.run([self.probs, self.final_state], feed)\n",
    "            #print probs\n",
    "            line = (np.sign(probs-0.5)+1)/2\n",
    "            #print line\n",
    "            picture_vect[n] = np.squeeze(line)  \n",
    "            picture = picture_vect*255\n",
    "        return tf.expand_dims(picture,2)\n",
    "    \n",
    "    def inspect(self, draw=False):\n",
    "        for var in tf.all_variables():\n",
    "            if var in tf.trainable_variables():\n",
    "                print ('t', var.name, var.eval().shape)\n",
    "                if draw:\n",
    "                    plt.figure(figsize=(1,1))\n",
    "                    plt.figimage(var.eval())\n",
    "                    plt.show()\n",
    "            else:\n",
    "                print ('nt', var.name, var.eval().shape)\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model created\n",
      "('faces count', 13233)\n",
      "variable initialized\n",
      "0/13233 (epoch 0), train_loss = 52.579899, time/batch = 0.933\n",
      "model saved to save_face_training/model.ckpt\n",
      "1/13233 (epoch 0), train_loss = 49.721397, time/batch = 0.493\n",
      "2/13233 (epoch 0), train_loss = 50.492420, time/batch = 0.500\n",
      "3/13233 (epoch 0), train_loss = 59.101929, time/batch = 0.482\n",
      "4/13233 (epoch 0), train_loss = 57.518559, time/batch = 0.493\n",
      "5/13233 (epoch 0), train_loss = 58.706413, time/batch = 0.487\n",
      "6/13233 (epoch 0), train_loss = 46.374111, time/batch = 0.502\n",
      "7/13233 (epoch 0), train_loss = 37.604885, time/batch = 0.485\n",
      "8/13233 (epoch 0), train_loss = 46.693913, time/batch = 0.500\n",
      "9/13233 (epoch 0), train_loss = 53.435909, time/batch = 0.476\n",
      "10/13233 (epoch 0), train_loss = 44.674656, time/batch = 0.516\n",
      "11/13233 (epoch 0), train_loss = 45.229965, time/batch = 0.485\n",
      "12/13233 (epoch 0), train_loss = 56.105579, time/batch = 0.507\n",
      "13/13233 (epoch 0), train_loss = 19.588402, time/batch = 0.489\n",
      "14/13233 (epoch 0), train_loss = 54.571098, time/batch = 0.501\n",
      "15/13233 (epoch 0), train_loss = 53.077190, time/batch = 0.488\n",
      "16/13233 (epoch 0), train_loss = 21.097343, time/batch = 0.503\n",
      "17/13233 (epoch 0), train_loss = 49.676529, time/batch = 0.496\n",
      "18/13233 (epoch 0), train_loss = 24.833286, time/batch = 0.500\n",
      "19/13233 (epoch 0), train_loss = 11.841604, time/batch = 0.489\n",
      "20/13233 (epoch 0), train_loss = 22.279915, time/batch = 0.481\n",
      "21/13233 (epoch 0), train_loss = 59.353638, time/batch = 0.488\n",
      "22/13233 (epoch 0), train_loss = 68.363670, time/batch = 0.523\n",
      "23/13233 (epoch 0), train_loss = 65.518684, time/batch = 0.497\n",
      "24/13233 (epoch 0), train_loss = 35.797459, time/batch = 0.501\n",
      "25/13233 (epoch 0), train_loss = 20.284224, time/batch = 0.495\n",
      "26/13233 (epoch 0), train_loss = 74.910149, time/batch = 0.497\n",
      "27/13233 (epoch 0), train_loss = 50.687153, time/batch = 0.501\n",
      "28/13233 (epoch 0), train_loss = 44.218582, time/batch = 0.519\n",
      "29/13233 (epoch 0), train_loss = 25.307335, time/batch = 0.494\n",
      "30/13233 (epoch 0), train_loss = 23.934795, time/batch = 0.514\n",
      "31/13233 (epoch 0), train_loss = 25.490742, time/batch = 0.494\n",
      "32/13233 (epoch 0), train_loss = 52.760612, time/batch = 0.496\n",
      "33/13233 (epoch 0), train_loss = 34.236744, time/batch = 0.496\n",
      "34/13233 (epoch 0), train_loss = 55.937115, time/batch = 0.525\n",
      "35/13233 (epoch 0), train_loss = 27.714531, time/batch = 0.494\n",
      "36/13233 (epoch 0), train_loss = 44.063900, time/batch = 0.508\n",
      "37/13233 (epoch 0), train_loss = 42.847992, time/batch = 0.500\n",
      "38/13233 (epoch 0), train_loss = 55.896603, time/batch = 0.502\n",
      "39/13233 (epoch 0), train_loss = 20.325678, time/batch = 0.499\n",
      "40/13233 (epoch 0), train_loss = 54.448044, time/batch = 0.517\n",
      "41/13233 (epoch 0), train_loss = 34.330070, time/batch = 0.495\n",
      "42/13233 (epoch 0), train_loss = 69.182655, time/batch = 0.511\n",
      "43/13233 (epoch 0), train_loss = 27.453808, time/batch = 0.503\n",
      "44/13233 (epoch 0), train_loss = 38.544621, time/batch = 0.494\n",
      "45/13233 (epoch 0), train_loss = 41.666836, time/batch = 0.503\n",
      "46/13233 (epoch 0), train_loss = 17.076517, time/batch = 0.526\n",
      "47/13233 (epoch 0), train_loss = 19.953030, time/batch = 0.498\n",
      "48/13233 (epoch 0), train_loss = 26.993387, time/batch = 0.538\n",
      "49/13233 (epoch 0), train_loss = 46.632362, time/batch = 0.533\n",
      "50/13233 (epoch 0), train_loss = 15.900846, time/batch = 0.533\n",
      "model saved to save_face_training/model.ckpt\n",
      "51/13233 (epoch 0), train_loss = 43.152901, time/batch = 0.515\n",
      "52/13233 (epoch 0), train_loss = 72.749695, time/batch = 0.531\n",
      "53/13233 (epoch 0), train_loss = 65.569153, time/batch = 0.497\n",
      "54/13233 (epoch 0), train_loss = 19.785519, time/batch = 0.518\n",
      "55/13233 (epoch 0), train_loss = 55.939281, time/batch = 0.498\n",
      "56/13233 (epoch 0), train_loss = 16.886316, time/batch = 0.512\n",
      "57/13233 (epoch 0), train_loss = 48.434956, time/batch = 0.505\n",
      "58/13233 (epoch 0), train_loss = 18.179176, time/batch = 0.521\n",
      "59/13233 (epoch 0), train_loss = 44.268475, time/batch = 0.503\n",
      "60/13233 (epoch 0), train_loss = 29.390104, time/batch = 0.511\n",
      "61/13233 (epoch 0), train_loss = 55.975929, time/batch = 0.501\n",
      "62/13233 (epoch 0), train_loss = 37.700726, time/batch = 0.505\n",
      "63/13233 (epoch 0), train_loss = 43.342327, time/batch = 0.501\n",
      "64/13233 (epoch 0), train_loss = 41.178043, time/batch = 0.523\n",
      "65/13233 (epoch 0), train_loss = 26.784037, time/batch = 0.503\n",
      "66/13233 (epoch 0), train_loss = 39.465153, time/batch = 0.509\n",
      "67/13233 (epoch 0), train_loss = 30.135256, time/batch = 0.683\n",
      "68/13233 (epoch 0), train_loss = 32.016804, time/batch = 0.503\n",
      "69/13233 (epoch 0), train_loss = 35.796021, time/batch = 0.511\n",
      "70/13233 (epoch 0), train_loss = 27.940187, time/batch = 0.501\n",
      "71/13233 (epoch 0), train_loss = 19.209160, time/batch = 0.511\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "args = Args()\n",
    "model = Model(args)\n",
    "print (\"model created\")\n",
    "faceloader = FaceLoader()\n",
    "face_count = faceloader.prepare_reading_faces()\n",
    "print ('faces count', face_count)\n",
    "\n",
    "cost_optimisation = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print (\"variable initialized\")\n",
    "    faceloader.do_when_session()\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "    # restore model\n",
    "    if args.init_from is not None:\n",
    "        ckpt = tf.train.get_checkpoint_state(args.init_from)\n",
    "        assert ckpt,\"No checkpoint found\"\n",
    "        assert ckpt.model_checkpoint_path,\"No model path found in checkpoint\"\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print (\"model restored\")\n",
    "    for e in range(args.num_epochs):\n",
    "        sess.run(tf.assign(model.lr, args.learning_rate * (args.decay_rate ** e)))\n",
    "        state = model.initial_state.eval()\n",
    "        for b in range(face_count):\n",
    "            start = time.time()\n",
    "            # Get learning data\n",
    "            faceloader.load_one_face(args.picture_size)\n",
    "            x, y = faceloader.next_batch()\n",
    "            # Create the structure for the learning data\n",
    "            feed = {model.input_data: x, model.target_data: y, model.initial_state: state}\n",
    "            # Run a session using train_op\n",
    "            [train_loss], state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\n",
    "            end = time.time()\n",
    "            if (e * face_count + b) % args.print_every == 0:\n",
    "                cost_optimisation.append(train_loss)\n",
    "                print(\"{}/{} (epoch {}), train_loss = {:.6f}, time/batch = {:.3f}\" \\\n",
    "                    .format(e * face_count + b,\n",
    "                            args.num_epochs * face_count,\n",
    "                            e, train_loss, end - start))\n",
    "            if (e * face_count + b) % args.save_every == 0:\n",
    "                checkpoint_path = os.path.join(args.save_dir, 'model.ckpt')\n",
    "                saver.save(sess, checkpoint_path, global_step = e * face_count + b)\n",
    "                print(\"model saved to {}\".format(checkpoint_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(range(len(cost_optimisation)), cost_optimisation, label='cost')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "args = Args()\n",
    "model = Model(args, True)  # True to generate the model in sampling mode\n",
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "    ckpt = tf.train.get_checkpoint_state(args.save_dir)\n",
    "    print (ckpt)\n",
    "    \n",
    "    model.inspect(draw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "args = Args()\n",
    "model = Model(args, infer=True)\n",
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print 'intialisation done'\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "    ckpt = tf.train.get_checkpoint_state(args.save_dir)\n",
    "    print (ckpt)\n",
    "    \n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "        pict = model.sample(sess)\n",
    "        print(pict.eval())\n",
    "        write_png(pict, 'a_face.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(\"a_face.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedback wellcome @dh7net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
