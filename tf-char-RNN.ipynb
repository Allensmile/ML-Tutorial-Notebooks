{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char RNN Tensorflow in Jupyter\n",
    "\n",
    "This Jupyter Notebook is based on [this code](https://github.com/sherjilozair/char-rnn-tensorflow) from **Sherjil Ozair**\n",
    "\n",
    "It implement RNN and LSTM at char level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import needed for Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "from tensorflow.python.ops import seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import needed for Jupiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports needed for utilities\n",
    "to load the text and transform it as a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "import collections\n",
    "from six.moves import cPickle\n",
    "from six import text_type\n",
    "import time\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "This Class need to be overided if you want to deal with other kind of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextLoader():\n",
    "    def __init__(self, data_dir, batch_size, seq_length, encoding='utf-8'):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.encoding = encoding\n",
    "\n",
    "        input_file = os.path.join(data_dir, \"input.txt\")\n",
    "        vocab_file = os.path.join(data_dir, \"vocab.pkl\")\n",
    "        tensor_file = os.path.join(data_dir, \"data.npy\")\n",
    "\n",
    "        if not (os.path.exists(vocab_file) and os.path.exists(tensor_file)):\n",
    "            print(\"reading text file\")\n",
    "            self.preprocess(input_file, vocab_file, tensor_file)\n",
    "        else:\n",
    "            print(\"loading preprocessed files\")\n",
    "            self.load_preprocessed(vocab_file, tensor_file)\n",
    "        self.create_batches()\n",
    "        self.reset_batch_pointer()\n",
    "\n",
    "    def preprocess(self, input_file, vocab_file, tensor_file):\n",
    "        with codecs.open(input_file, \"r\", encoding=self.encoding) as f:\n",
    "            data = f.read()\n",
    "        counter = collections.Counter(data)\n",
    "        count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "        self.chars, _ = zip(*count_pairs)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        with open(vocab_file, 'wb') as f:\n",
    "            cPickle.dump(self.chars, f)\n",
    "        self.tensor = np.array(list(map(self.vocab.get, data)))\n",
    "        np.save(tensor_file, self.tensor)\n",
    "\n",
    "    def load_preprocessed(self, vocab_file, tensor_file):\n",
    "        with open(vocab_file, 'rb') as f:\n",
    "            self.chars = cPickle.load(f)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        self.tensor = np.load(tensor_file)\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size *\n",
    "                                                   self.seq_length))\n",
    "\n",
    "    def create_batches(self):\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size *\n",
    "                                                   self.seq_length))\n",
    "\n",
    "        # When the data (tesor) is too small, let's give them a better error message\n",
    "        if self.num_batches==0:\n",
    "            assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
    "\n",
    "        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]\n",
    "        xdata = self.tensor\n",
    "        ydata = np.copy(self.tensor)\n",
    "        ydata[:-1] = xdata[1:]\n",
    "        ydata[-1] = xdata[0]\n",
    "        self.x_batches = np.split(xdata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x, y\n",
    "\n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args, to define all parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        '''data directory containing input.txt'''\n",
    "        self.data_dir = 'data_rnn/tinyshakespeare'\n",
    "        '''directory to store checkpointed models'''\n",
    "        \n",
    "        self.save_dir = 'save'\n",
    "        '''size of RNN hidden state'''\n",
    "        self.rnn_size = 128\n",
    "        '''number of layers in the RNN'''\n",
    "        self.num_layers = 2\n",
    "        '''rnn, gru, or lstm'''\n",
    "        self.model = 'lstm'\n",
    "        '''minibatch size'''\n",
    "        self.batch_size = 50\n",
    "        '''RNN sequence length'''\n",
    "        self.seq_length = 50\n",
    "        '''number of epochs'''\n",
    "        self.num_epochs = 5\n",
    "        '''save frequency'''\n",
    "        self.save_every = 500\n",
    "        '''clip gradients at this value'''\n",
    "        self.grad_clip = 5.\n",
    "        '''learning rate'''\n",
    "        self.learning_rate = 0.002\n",
    "        '''decay rate for rmsprop'''\n",
    "        self.decay_rate = 0.97\n",
    "        \"\"\"continue training from saved model at this path. Path must contain files saved by previous training process: \n",
    "                            'config.pkl'        : configuration;\n",
    "                            'chars_vocab.pkl'   : vocabulary definitions;\n",
    "                            'checkpoint'        : paths to model file(s) (created by tf).\n",
    "                                                  Note: this file contains absolute paths, be careful when moving files around;\n",
    "                            'model.ckpt-*'      : file(s) with model definition (created by tf)\n",
    "                        \"\"\"\n",
    "        self.init_from = 'save'\n",
    "        \n",
    "        \n",
    "        '''number of characters to sample'''\n",
    "        self.n = 500\n",
    "        '''prime text'''\n",
    "        self.prime = u' '\n",
    "        '''0 to use max at each timestep, 1 to sample at each timestep, 2 to sample on spaces'''\n",
    "        self.sample = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, args, infer=False):\n",
    "        self.args = args\n",
    "        if infer:\n",
    "            args.batch_size = 1\n",
    "            args.seq_length = 1\n",
    "\n",
    "        if args.model == 'rnn':\n",
    "            cell_fn = rnn_cell.BasicRNNCell\n",
    "        elif args.model == 'gru':\n",
    "            cell_fn = rnn_cell.GRUCell\n",
    "        elif args.model == 'lstm':\n",
    "            cell_fn = rnn_cell.BasicLSTMCell\n",
    "        else:\n",
    "            raise Exception(\"model type not supported: {}\".format(args.model))\n",
    "\n",
    "        cell = cell_fn(args.rnn_size)\n",
    "\n",
    "        self.cell = cell = rnn_cell.MultiRNNCell([cell] * args.num_layers)\n",
    "\n",
    "        self.input_data = tf.placeholder(tf.int32, [args.batch_size, args.seq_length])\n",
    "        self.targets = tf.placeholder(tf.int32, [args.batch_size, args.seq_length])\n",
    "        self.initial_state = cell.zero_state(args.batch_size, tf.float32)\n",
    "\n",
    "        with tf.variable_scope('rnnlm'):\n",
    "            softmax_w = tf.get_variable(\"softmax_w\", [args.rnn_size, args.vocab_size])\n",
    "            softmax_b = tf.get_variable(\"softmax_b\", [args.vocab_size])\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                embedding = tf.get_variable(\"embedding\", [args.vocab_size, args.rnn_size])\n",
    "                inputs = tf.split(1, args.seq_length, tf.nn.embedding_lookup(embedding, self.input_data))\n",
    "                inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "\n",
    "        def loop(prev, _):\n",
    "            prev = tf.matmul(prev, softmax_w) + softmax_b\n",
    "            prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n",
    "            return tf.nn.embedding_lookup(embedding, prev_symbol)\n",
    "\n",
    "        outputs, last_state = seq2seq.rnn_decoder(inputs, self.initial_state, cell, loop_function=loop if infer else None, scope='rnnlm')\n",
    "        output = tf.reshape(tf.concat(1, outputs), [-1, args.rnn_size])\n",
    "        self.logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        self.probs = tf.nn.softmax(self.logits)\n",
    "        loss = seq2seq.sequence_loss_by_example([self.logits],\n",
    "                [tf.reshape(self.targets, [-1])],\n",
    "                [tf.ones([args.batch_size * args.seq_length])],\n",
    "                args.vocab_size)\n",
    "        self.cost = tf.reduce_sum(loss) / args.batch_size / args.seq_length\n",
    "        self.final_state = last_state\n",
    "        self.lr = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),\n",
    "                args.grad_clip)\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    def sample(self, sess, chars, vocab, num=200, prime='The ', sampling_type=1):\n",
    "        state = self.cell.zero_state(1, tf.float32).eval()\n",
    "        for char in prime[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [state] = sess.run([self.final_state], feed)\n",
    "\n",
    "        def weighted_pick(weights):\n",
    "            t = np.cumsum(weights)\n",
    "            s = np.sum(weights)\n",
    "            return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "        ret = prime\n",
    "        char = prime[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [probs, state] = sess.run([self.probs, self.final_state], feed)\n",
    "            p = probs[0]\n",
    "\n",
    "            if sampling_type == 0:\n",
    "                sample = np.argmax(p)\n",
    "            elif sampling_type == 2:\n",
    "                if char == ' ':\n",
    "                    sample = weighted_pick(p)\n",
    "                else:\n",
    "                    sample = np.argmax(p)\n",
    "            else: # sampling_type == 1 default:\n",
    "                sample = weighted_pick(p)\n",
    "\n",
    "            pred = chars[sample]\n",
    "            ret += pred\n",
    "            char = pred\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    data_loader = TextLoader(args.data_dir, args.batch_size, args.seq_length)\n",
    "    args.vocab_size = data_loader.vocab_size\n",
    "    print (args.vocab_size)\n",
    "    \n",
    "    # check compatibility if training is continued from previously saved model\n",
    "    if args.init_from is not None:\n",
    "        print (\"need to load file from\", args.init_from)\n",
    "        # check if all necessary files exist \n",
    "        assert os.path.isdir(args.init_from),\" %s must be a a path\" % args.init_from\n",
    "        assert os.path.isfile(os.path.join(args.init_from,\"config.pkl\")),\"config.pkl file does not exist in path %s\"%args.init_from\n",
    "        assert os.path.isfile(os.path.join(args.init_from,\"chars_vocab.pkl\")),\"chars_vocab.pkl.pkl file does not exist in path %s\" % args.init_from\n",
    "        ckpt = tf.train.get_checkpoint_state(args.init_from)\n",
    "        assert ckpt,\"No checkpoint found\"\n",
    "        assert ckpt.model_checkpoint_path,\"No model path found in checkpoint\"\n",
    "\n",
    "        # open old config and check if models are compatible\n",
    "        with open(os.path.join(args.init_from, 'config.pkl')) as f:\n",
    "            saved_model_args = cPickle.load(f)\n",
    "        need_be_same=[\"model\",\"rnn_size\",\"num_layers\",\"seq_length\"]\n",
    "        for checkme in need_be_same:\n",
    "            assert vars(saved_model_args)[checkme]==vars(args)[checkme],\"Command line argument and saved model disagree on '%s' \"%checkme\n",
    "        \n",
    "        # open saved vocab/dict and check if vocabs/dicts are compatible\n",
    "        with open(os.path.join(args.init_from, 'chars_vocab.pkl')) as f:\n",
    "            saved_chars, saved_vocab = cPickle.load(f)\n",
    "        assert saved_chars==data_loader.chars, \"Data and loaded model disagreee on character set!\"\n",
    "        assert saved_vocab==data_loader.vocab, \"Data and loaded model disagreee on dictionary mappings!\"\n",
    "        print (\"config loaded\")\n",
    "        \n",
    "    with open(os.path.join(args.save_dir, 'config.pkl'), 'wb') as f:\n",
    "        cPickle.dump(args, f)\n",
    "    with open(os.path.join(args.save_dir, 'chars_vocab.pkl'), 'wb') as f:\n",
    "        cPickle.dump((data_loader.chars, data_loader.vocab), f)\n",
    "        \n",
    "    model = Model(args)\n",
    "    print (\"model created\")\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print (\"variable initialized\")\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "        # restore model\n",
    "        if args.init_from is not None:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            print (\"model restored\")\n",
    "        for e in range(args.num_epochs):\n",
    "            sess.run(tf.assign(model.lr, args.learning_rate * (args.decay_rate ** e)))\n",
    "            data_loader.reset_batch_pointer()\n",
    "            state = model.initial_state.eval()\n",
    "            for b in range(data_loader.num_batches):\n",
    "                start = time.time()\n",
    "                x, y = data_loader.next_batch()\n",
    "                feed = {model.input_data: x, model.targets: y, model.initial_state: state}\n",
    "                train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\n",
    "                end = time.time()\n",
    "                print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "                    .format(e * data_loader.num_batches + b,\n",
    "                            args.num_epochs * data_loader.num_batches,\n",
    "                            e, train_loss, end - start))\n",
    "                if (e * data_loader.num_batches + b) % args.save_every == 0\\\n",
    "                    or (e==args.num_epochs-1 and b == data_loader.num_batches-1): # save for the last result\n",
    "                    checkpoint_path = os.path.join(args.save_dir, 'model.ckpt')\n",
    "                    saver.save(sess, checkpoint_path, global_step = e * data_loader.num_batches + b)\n",
    "                    print(\"model saved to {}\".format(checkpoint_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample(args):\n",
    "    with open(os.path.join(args.save_dir, 'config.pkl'), 'rb') as f:\n",
    "        saved_args = cPickle.load(f)\n",
    "    with open(os.path.join(args.save_dir, 'chars_vocab.pkl'), 'rb') as f:\n",
    "        chars, vocab = cPickle.load(f)\n",
    "    model = Model(saved_args, True)\n",
    "    with tf.Session() as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "        ckpt = tf.train.get_checkpoint_state(args.save_dir)\n",
    "        print (ckpt)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            print(model.sample(sess, chars, vocab, args.n, args.prime, args.sample))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying the code :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading preprocessed files\n",
      "65\n",
      "need to load file from save\n",
      "config loaded\n",
      "model created\n",
      "variable initialized\n",
      "model restored\n",
      "0/2230 (epoch 0), train_loss = 4.474, time/batch = 1.242\n",
      "model saved to save/model.ckpt\n",
      "1/2230 (epoch 0), train_loss = 4.129, time/batch = 0.607\n",
      "2/2230 (epoch 0), train_loss = 3.940, time/batch = 0.629\n",
      "3/2230 (epoch 0), train_loss = 3.827, time/batch = 0.618\n",
      "4/2230 (epoch 0), train_loss = 3.647, time/batch = 0.583\n",
      "5/2230 (epoch 0), train_loss = 3.576, time/batch = 0.623\n",
      "6/2230 (epoch 0), train_loss = 3.494, time/batch = 0.605\n",
      "7/2230 (epoch 0), train_loss = 3.409, time/batch = 0.582\n",
      "8/2230 (epoch 0), train_loss = 3.355, time/batch = 0.605\n",
      "9/2230 (epoch 0), train_loss = 3.432, time/batch = 0.589\n",
      "10/2230 (epoch 0), train_loss = 3.329, time/batch = 0.598\n",
      "11/2230 (epoch 0), train_loss = 3.362, time/batch = 0.597\n",
      "12/2230 (epoch 0), train_loss = 3.342, time/batch = 0.588\n",
      "13/2230 (epoch 0), train_loss = 3.328, time/batch = 0.612\n",
      "14/2230 (epoch 0), train_loss = 3.404, time/batch = 0.614\n",
      "15/2230 (epoch 0), train_loss = 3.372, time/batch = 0.617\n",
      "16/2230 (epoch 0), train_loss = 3.317, time/batch = 0.601\n",
      "17/2230 (epoch 0), train_loss = 3.331, time/batch = 0.647\n",
      "18/2230 (epoch 0), train_loss = 3.310, time/batch = 0.607\n",
      "19/2230 (epoch 0), train_loss = 3.277, time/batch = 0.594\n",
      "20/2230 (epoch 0), train_loss = 3.324, time/batch = 0.640\n",
      "21/2230 (epoch 0), train_loss = 3.347, time/batch = 0.619\n",
      "22/2230 (epoch 0), train_loss = 3.352, time/batch = 0.603\n",
      "23/2230 (epoch 0), train_loss = 3.353, time/batch = 0.599\n",
      "24/2230 (epoch 0), train_loss = 3.403, time/batch = 0.589\n",
      "25/2230 (epoch 0), train_loss = 3.349, time/batch = 0.591\n",
      "26/2230 (epoch 0), train_loss = 3.293, time/batch = 0.592\n",
      "27/2230 (epoch 0), train_loss = 3.284, time/batch = 0.591\n",
      "28/2230 (epoch 0), train_loss = 3.280, time/batch = 0.591\n",
      "29/2230 (epoch 0), train_loss = 3.286, time/batch = 0.602\n",
      "30/2230 (epoch 0), train_loss = 3.331, time/batch = 0.596\n",
      "31/2230 (epoch 0), train_loss = 3.359, time/batch = 0.595\n",
      "32/2230 (epoch 0), train_loss = 3.283, time/batch = 0.575\n",
      "33/2230 (epoch 0), train_loss = 3.310, time/batch = 0.612\n",
      "34/2230 (epoch 0), train_loss = 3.297, time/batch = 0.600\n",
      "35/2230 (epoch 0), train_loss = 3.297, time/batch = 0.759\n",
      "36/2230 (epoch 0), train_loss = 3.300, time/batch = 0.645\n",
      "37/2230 (epoch 0), train_loss = 3.261, time/batch = 0.624\n",
      "38/2230 (epoch 0), train_loss = 3.319, time/batch = 0.623\n",
      "39/2230 (epoch 0), train_loss = 3.316, time/batch = 0.708\n",
      "40/2230 (epoch 0), train_loss = 3.323, time/batch = 0.701\n",
      "41/2230 (epoch 0), train_loss = 3.222, time/batch = 0.678\n",
      "42/2230 (epoch 0), train_loss = 3.267, time/batch = 0.727\n",
      "43/2230 (epoch 0), train_loss = 3.224, time/batch = 0.686\n",
      "44/2230 (epoch 0), train_loss = 3.301, time/batch = 0.876\n",
      "45/2230 (epoch 0), train_loss = 3.271, time/batch = 0.790\n",
      "46/2230 (epoch 0), train_loss = 3.284, time/batch = 0.794\n",
      "47/2230 (epoch 0), train_loss = 3.324, time/batch = 0.781\n",
      "48/2230 (epoch 0), train_loss = 3.249, time/batch = 0.721\n",
      "49/2230 (epoch 0), train_loss = 3.297, time/batch = 0.707\n",
      "50/2230 (epoch 0), train_loss = 3.272, time/batch = 0.791\n",
      "51/2230 (epoch 0), train_loss = 3.227, time/batch = 0.700\n",
      "52/2230 (epoch 0), train_loss = 3.256, time/batch = 0.746\n",
      "53/2230 (epoch 0), train_loss = 3.198, time/batch = 0.730\n",
      "54/2230 (epoch 0), train_loss = 3.201, time/batch = 0.727\n",
      "55/2230 (epoch 0), train_loss = 3.216, time/batch = 0.741\n",
      "56/2230 (epoch 0), train_loss = 3.160, time/batch = 0.855\n",
      "57/2230 (epoch 0), train_loss = 3.128, time/batch = 0.701\n",
      "58/2230 (epoch 0), train_loss = 3.127, time/batch = 0.688\n",
      "59/2230 (epoch 0), train_loss = 3.126, time/batch = 0.672\n",
      "60/2230 (epoch 0), train_loss = 3.130, time/batch = 0.719\n",
      "61/2230 (epoch 0), train_loss = 3.170, time/batch = 0.750\n",
      "62/2230 (epoch 0), train_loss = 3.091, time/batch = 0.698\n",
      "63/2230 (epoch 0), train_loss = 3.123, time/batch = 0.712\n",
      "64/2230 (epoch 0), train_loss = 3.110, time/batch = 0.707\n",
      "65/2230 (epoch 0), train_loss = 3.003, time/batch = 0.718\n",
      "66/2230 (epoch 0), train_loss = 3.034, time/batch = 0.699\n",
      "67/2230 (epoch 0), train_loss = 3.002, time/batch = 0.706\n",
      "68/2230 (epoch 0), train_loss = 3.016, time/batch = 0.695\n",
      "69/2230 (epoch 0), train_loss = 2.985, time/batch = 0.694\n",
      "70/2230 (epoch 0), train_loss = 2.981, time/batch = 0.725\n",
      "71/2230 (epoch 0), train_loss = 3.001, time/batch = 0.643\n",
      "72/2230 (epoch 0), train_loss = 2.943, time/batch = 0.731\n",
      "73/2230 (epoch 0), train_loss = 2.914, time/batch = 0.771\n",
      "74/2230 (epoch 0), train_loss = 2.950, time/batch = 0.870\n",
      "75/2230 (epoch 0), train_loss = 2.885, time/batch = 0.700\n",
      "76/2230 (epoch 0), train_loss = 2.892, time/batch = 0.708\n",
      "77/2230 (epoch 0), train_loss = 2.907, time/batch = 0.757\n",
      "78/2230 (epoch 0), train_loss = 2.919, time/batch = 0.746\n",
      "79/2230 (epoch 0), train_loss = 2.866, time/batch = 0.740\n",
      "80/2230 (epoch 0), train_loss = 2.893, time/batch = 0.723\n",
      "81/2230 (epoch 0), train_loss = 2.836, time/batch = 0.685\n",
      "82/2230 (epoch 0), train_loss = 2.872, time/batch = 0.679\n",
      "83/2230 (epoch 0), train_loss = 2.822, time/batch = 0.672\n",
      "84/2230 (epoch 0), train_loss = 2.841, time/batch = 0.679\n",
      "85/2230 (epoch 0), train_loss = 2.803, time/batch = 0.636\n",
      "86/2230 (epoch 0), train_loss = 2.794, time/batch = 0.719\n",
      "87/2230 (epoch 0), train_loss = 2.741, time/batch = 0.651\n",
      "88/2230 (epoch 0), train_loss = 2.816, time/batch = 0.670\n",
      "89/2230 (epoch 0), train_loss = 2.740, time/batch = 0.905\n",
      "90/2230 (epoch 0), train_loss = 2.721, time/batch = 0.882\n",
      "91/2230 (epoch 0), train_loss = 2.702, time/batch = 0.754\n",
      "92/2230 (epoch 0), train_loss = 2.677, time/batch = 0.708\n",
      "93/2230 (epoch 0), train_loss = 2.693, time/batch = 0.693\n",
      "94/2230 (epoch 0), train_loss = 2.761, time/batch = 0.700\n",
      "95/2230 (epoch 0), train_loss = 2.704, time/batch = 0.646\n",
      "96/2230 (epoch 0), train_loss = 2.731, time/batch = 0.810\n",
      "97/2230 (epoch 0), train_loss = 2.670, time/batch = 0.769\n",
      "98/2230 (epoch 0), train_loss = 2.695, time/batch = 0.686\n",
      "99/2230 (epoch 0), train_loss = 2.689, time/batch = 0.690\n",
      "100/2230 (epoch 0), train_loss = 2.676, time/batch = 0.703\n",
      "101/2230 (epoch 0), train_loss = 2.627, time/batch = 0.650\n",
      "102/2230 (epoch 0), train_loss = 2.679, time/batch = 0.581\n",
      "103/2230 (epoch 0), train_loss = 2.606, time/batch = 0.623\n",
      "104/2230 (epoch 0), train_loss = 2.580, time/batch = 0.594\n",
      "105/2230 (epoch 0), train_loss = 2.582, time/batch = 0.595\n",
      "106/2230 (epoch 0), train_loss = 2.590, time/batch = 0.611\n",
      "107/2230 (epoch 0), train_loss = 2.591, time/batch = 0.581\n",
      "108/2230 (epoch 0), train_loss = 2.569, time/batch = 0.619\n",
      "109/2230 (epoch 0), train_loss = 2.558, time/batch = 0.604\n",
      "110/2230 (epoch 0), train_loss = 2.533, time/batch = 0.593\n",
      "111/2230 (epoch 0), train_loss = 2.581, time/batch = 0.625\n",
      "112/2230 (epoch 0), train_loss = 2.572, time/batch = 0.604\n",
      "113/2230 (epoch 0), train_loss = 2.531, time/batch = 0.591\n",
      "114/2230 (epoch 0), train_loss = 2.547, time/batch = 0.605\n",
      "115/2230 (epoch 0), train_loss = 2.528, time/batch = 0.588\n",
      "116/2230 (epoch 0), train_loss = 2.548, time/batch = 0.599\n",
      "117/2230 (epoch 0), train_loss = 2.524, time/batch = 0.577\n",
      "118/2230 (epoch 0), train_loss = 2.481, time/batch = 0.653\n",
      "119/2230 (epoch 0), train_loss = 2.479, time/batch = 0.629\n",
      "120/2230 (epoch 0), train_loss = 2.487, time/batch = 0.651\n",
      "121/2230 (epoch 0), train_loss = 2.460, time/batch = 0.685\n",
      "122/2230 (epoch 0), train_loss = 2.483, time/batch = 0.656\n",
      "123/2230 (epoch 0), train_loss = 2.444, time/batch = 0.629\n",
      "124/2230 (epoch 0), train_loss = 2.465, time/batch = 0.645\n",
      "125/2230 (epoch 0), train_loss = 2.498, time/batch = 0.676\n",
      "126/2230 (epoch 0), train_loss = 2.465, time/batch = 0.634\n",
      "127/2230 (epoch 0), train_loss = 2.447, time/batch = 0.651\n",
      "128/2230 (epoch 0), train_loss = 2.475, time/batch = 0.586\n",
      "129/2230 (epoch 0), train_loss = 2.414, time/batch = 0.582\n",
      "130/2230 (epoch 0), train_loss = 2.455, time/batch = 0.581\n",
      "131/2230 (epoch 0), train_loss = 2.410, time/batch = 0.587\n",
      "132/2230 (epoch 0), train_loss = 2.469, time/batch = 0.598\n",
      "133/2230 (epoch 0), train_loss = 2.409, time/batch = 0.582\n",
      "134/2230 (epoch 0), train_loss = 2.378, time/batch = 0.589\n",
      "135/2230 (epoch 0), train_loss = 2.431, time/batch = 0.664\n",
      "136/2230 (epoch 0), train_loss = 2.440, time/batch = 0.608\n",
      "137/2230 (epoch 0), train_loss = 2.443, time/batch = 0.718\n",
      "138/2230 (epoch 0), train_loss = 2.432, time/batch = 0.704\n",
      "139/2230 (epoch 0), train_loss = 2.466, time/batch = 0.664\n",
      "140/2230 (epoch 0), train_loss = 2.444, time/batch = 0.679\n",
      "141/2230 (epoch 0), train_loss = 2.437, time/batch = 0.758\n",
      "142/2230 (epoch 0), train_loss = 2.366, time/batch = 0.711\n",
      "143/2230 (epoch 0), train_loss = 2.375, time/batch = 0.680\n",
      "144/2230 (epoch 0), train_loss = 2.395, time/batch = 0.756\n",
      "145/2230 (epoch 0), train_loss = 2.383, time/batch = 0.694\n",
      "146/2230 (epoch 0), train_loss = 2.373, time/batch = 0.592\n",
      "147/2230 (epoch 0), train_loss = 2.351, time/batch = 0.630\n",
      "148/2230 (epoch 0), train_loss = 2.420, time/batch = 0.582\n",
      "149/2230 (epoch 0), train_loss = 2.389, time/batch = 0.586\n",
      "150/2230 (epoch 0), train_loss = 2.360, time/batch = 0.617\n",
      "151/2230 (epoch 0), train_loss = 2.365, time/batch = 0.589\n",
      "152/2230 (epoch 0), train_loss = 2.330, time/batch = 0.603\n",
      "153/2230 (epoch 0), train_loss = 2.380, time/batch = 0.592\n",
      "154/2230 (epoch 0), train_loss = 2.337, time/batch = 0.573\n",
      "155/2230 (epoch 0), train_loss = 2.347, time/batch = 0.590\n",
      "156/2230 (epoch 0), train_loss = 2.316, time/batch = 0.592\n",
      "157/2230 (epoch 0), train_loss = 2.385, time/batch = 0.584\n",
      "158/2230 (epoch 0), train_loss = 2.339, time/batch = 0.567\n",
      "159/2230 (epoch 0), train_loss = 2.345, time/batch = 0.592\n",
      "160/2230 (epoch 0), train_loss = 2.313, time/batch = 0.594\n",
      "161/2230 (epoch 0), train_loss = 2.392, time/batch = 0.584\n",
      "162/2230 (epoch 0), train_loss = 2.356, time/batch = 0.572\n",
      "163/2230 (epoch 0), train_loss = 2.331, time/batch = 0.576\n",
      "164/2230 (epoch 0), train_loss = 2.366, time/batch = 0.589\n",
      "165/2230 (epoch 0), train_loss = 2.354, time/batch = 0.586\n",
      "166/2230 (epoch 0), train_loss = 2.327, time/batch = 0.610\n",
      "167/2230 (epoch 0), train_loss = 2.306, time/batch = 0.579\n",
      "168/2230 (epoch 0), train_loss = 2.335, time/batch = 0.581\n",
      "169/2230 (epoch 0), train_loss = 2.357, time/batch = 0.589\n",
      "170/2230 (epoch 0), train_loss = 2.354, time/batch = 0.567\n",
      "171/2230 (epoch 0), train_loss = 2.354, time/batch = 0.603\n",
      "172/2230 (epoch 0), train_loss = 2.340, time/batch = 0.573\n",
      "173/2230 (epoch 0), train_loss = 2.310, time/batch = 0.608\n",
      "174/2230 (epoch 0), train_loss = 2.325, time/batch = 0.623\n",
      "175/2230 (epoch 0), train_loss = 2.358, time/batch = 0.605\n",
      "176/2230 (epoch 0), train_loss = 2.361, time/batch = 0.658\n",
      "177/2230 (epoch 0), train_loss = 2.326, time/batch = 0.634\n",
      "178/2230 (epoch 0), train_loss = 2.314, time/batch = 0.608\n",
      "179/2230 (epoch 0), train_loss = 2.272, time/batch = 0.716\n",
      "180/2230 (epoch 0), train_loss = 2.287, time/batch = 0.628\n",
      "181/2230 (epoch 0), train_loss = 2.385, time/batch = 0.681\n",
      "182/2230 (epoch 0), train_loss = 2.316, time/batch = 0.675\n",
      "183/2230 (epoch 0), train_loss = 2.313, time/batch = 0.712\n",
      "184/2230 (epoch 0), train_loss = 2.246, time/batch = 0.701\n",
      "185/2230 (epoch 0), train_loss = 2.285, time/batch = 0.820\n",
      "186/2230 (epoch 0), train_loss = 2.274, time/batch = 0.759\n",
      "187/2230 (epoch 0), train_loss = 2.305, time/batch = 0.641\n",
      "188/2230 (epoch 0), train_loss = 2.285, time/batch = 0.709\n",
      "189/2230 (epoch 0), train_loss = 2.210, time/batch = 0.790\n",
      "190/2230 (epoch 0), train_loss = 2.282, time/batch = 0.631\n",
      "191/2230 (epoch 0), train_loss = 2.218, time/batch = 0.604\n",
      "192/2230 (epoch 0), train_loss = 2.273, time/batch = 0.745\n",
      "193/2230 (epoch 0), train_loss = 2.252, time/batch = 0.698\n",
      "194/2230 (epoch 0), train_loss = 2.268, time/batch = 0.831\n",
      "195/2230 (epoch 0), train_loss = 2.254, time/batch = 0.759\n",
      "196/2230 (epoch 0), train_loss = 2.252, time/batch = 0.668\n",
      "197/2230 (epoch 0), train_loss = 2.280, time/batch = 0.672\n",
      "198/2230 (epoch 0), train_loss = 2.307, time/batch = 0.664\n",
      "199/2230 (epoch 0), train_loss = 2.275, time/batch = 0.593\n",
      "200/2230 (epoch 0), train_loss = 2.320, time/batch = 0.622\n",
      "201/2230 (epoch 0), train_loss = 2.282, time/batch = 0.676\n",
      "202/2230 (epoch 0), train_loss = 2.278, time/batch = 0.670\n",
      "203/2230 (epoch 0), train_loss = 2.282, time/batch = 0.575\n",
      "204/2230 (epoch 0), train_loss = 2.257, time/batch = 0.566\n",
      "205/2230 (epoch 0), train_loss = 2.282, time/batch = 0.577\n",
      "206/2230 (epoch 0), train_loss = 2.247, time/batch = 0.596\n",
      "207/2230 (epoch 0), train_loss = 2.278, time/batch = 0.603\n",
      "208/2230 (epoch 0), train_loss = 2.326, time/batch = 0.575\n",
      "209/2230 (epoch 0), train_loss = 2.264, time/batch = 0.631\n",
      "210/2230 (epoch 0), train_loss = 2.259, time/batch = 0.627\n",
      "211/2230 (epoch 0), train_loss = 2.239, time/batch = 0.567\n",
      "212/2230 (epoch 0), train_loss = 2.248, time/batch = 0.642\n",
      "213/2230 (epoch 0), train_loss = 2.280, time/batch = 0.681\n",
      "214/2230 (epoch 0), train_loss = 2.197, time/batch = 0.574\n",
      "215/2230 (epoch 0), train_loss = 2.225, time/batch = 0.593\n",
      "216/2230 (epoch 0), train_loss = 2.255, time/batch = 0.595\n",
      "217/2230 (epoch 0), train_loss = 2.232, time/batch = 0.620\n",
      "218/2230 (epoch 0), train_loss = 2.229, time/batch = 0.609\n",
      "219/2230 (epoch 0), train_loss = 2.237, time/batch = 0.564\n",
      "220/2230 (epoch 0), train_loss = 2.220, time/batch = 0.627\n",
      "221/2230 (epoch 0), train_loss = 2.234, time/batch = 0.591\n",
      "222/2230 (epoch 0), train_loss = 2.237, time/batch = 0.593\n",
      "223/2230 (epoch 0), train_loss = 2.198, time/batch = 0.580\n",
      "224/2230 (epoch 0), train_loss = 2.197, time/batch = 0.580\n",
      "225/2230 (epoch 0), train_loss = 2.209, time/batch = 0.584\n",
      "226/2230 (epoch 0), train_loss = 2.300, time/batch = 0.570\n",
      "227/2230 (epoch 0), train_loss = 2.239, time/batch = 0.588\n",
      "228/2230 (epoch 0), train_loss = 2.244, time/batch = 0.573\n",
      "229/2230 (epoch 0), train_loss = 2.202, time/batch = 0.588\n",
      "230/2230 (epoch 0), train_loss = 2.241, time/batch = 0.578\n",
      "231/2230 (epoch 0), train_loss = 2.249, time/batch = 0.571\n",
      "232/2230 (epoch 0), train_loss = 2.212, time/batch = 0.653\n",
      "233/2230 (epoch 0), train_loss = 2.237, time/batch = 0.634\n",
      "234/2230 (epoch 0), train_loss = 2.204, time/batch = 0.584\n",
      "235/2230 (epoch 0), train_loss = 2.234, time/batch = 0.579\n",
      "236/2230 (epoch 0), train_loss = 2.221, time/batch = 0.571\n",
      "237/2230 (epoch 0), train_loss = 2.214, time/batch = 0.591\n",
      "238/2230 (epoch 0), train_loss = 2.209, time/batch = 0.566\n",
      "239/2230 (epoch 0), train_loss = 2.208, time/batch = 0.592\n",
      "240/2230 (epoch 0), train_loss = 2.208, time/batch = 0.580\n",
      "241/2230 (epoch 0), train_loss = 2.224, time/batch = 0.576\n",
      "242/2230 (epoch 0), train_loss = 2.195, time/batch = 0.588\n",
      "243/2230 (epoch 0), train_loss = 2.205, time/batch = 0.567\n",
      "244/2230 (epoch 0), train_loss = 2.175, time/batch = 0.585\n",
      "245/2230 (epoch 0), train_loss = 2.186, time/batch = 0.569\n",
      "246/2230 (epoch 0), train_loss = 2.191, time/batch = 0.571\n",
      "247/2230 (epoch 0), train_loss = 2.198, time/batch = 0.579\n",
      "248/2230 (epoch 0), train_loss = 2.214, time/batch = 0.591\n",
      "249/2230 (epoch 0), train_loss = 2.175, time/batch = 0.627\n",
      "250/2230 (epoch 0), train_loss = 2.244, time/batch = 0.587\n",
      "251/2230 (epoch 0), train_loss = 2.152, time/batch = 0.613\n",
      "252/2230 (epoch 0), train_loss = 2.214, time/batch = 0.595\n",
      "253/2230 (epoch 0), train_loss = 2.194, time/batch = 0.587\n",
      "254/2230 (epoch 0), train_loss = 2.176, time/batch = 0.594\n",
      "255/2230 (epoch 0), train_loss = 2.180, time/batch = 0.567\n",
      "256/2230 (epoch 0), train_loss = 2.219, time/batch = 0.657\n",
      "257/2230 (epoch 0), train_loss = 2.179, time/batch = 0.654\n",
      "258/2230 (epoch 0), train_loss = 2.224, time/batch = 0.575\n",
      "259/2230 (epoch 0), train_loss = 2.172, time/batch = 0.611\n",
      "260/2230 (epoch 0), train_loss = 2.176, time/batch = 0.565\n",
      "261/2230 (epoch 0), train_loss = 2.153, time/batch = 0.608\n",
      "262/2230 (epoch 0), train_loss = 2.167, time/batch = 0.618\n",
      "263/2230 (epoch 0), train_loss = 2.166, time/batch = 0.591\n",
      "264/2230 (epoch 0), train_loss = 2.181, time/batch = 0.588\n",
      "265/2230 (epoch 0), train_loss = 2.196, time/batch = 0.577\n",
      "266/2230 (epoch 0), train_loss = 2.234, time/batch = 0.588\n",
      "267/2230 (epoch 0), train_loss = 2.181, time/batch = 0.581\n",
      "268/2230 (epoch 0), train_loss = 2.120, time/batch = 0.579\n",
      "269/2230 (epoch 0), train_loss = 2.171, time/batch = 0.571\n",
      "270/2230 (epoch 0), train_loss = 2.157, time/batch = 0.564\n",
      "271/2230 (epoch 0), train_loss = 2.214, time/batch = 0.604\n",
      "272/2230 (epoch 0), train_loss = 2.149, time/batch = 0.574\n",
      "273/2230 (epoch 0), train_loss = 2.175, time/batch = 0.604\n",
      "274/2230 (epoch 0), train_loss = 2.143, time/batch = 0.594\n",
      "275/2230 (epoch 0), train_loss = 2.171, time/batch = 0.578\n",
      "276/2230 (epoch 0), train_loss = 2.150, time/batch = 0.590\n",
      "277/2230 (epoch 0), train_loss = 2.173, time/batch = 0.567\n",
      "278/2230 (epoch 0), train_loss = 2.187, time/batch = 0.601\n",
      "279/2230 (epoch 0), train_loss = 2.194, time/batch = 0.605\n",
      "280/2230 (epoch 0), train_loss = 2.121, time/batch = 0.646\n",
      "281/2230 (epoch 0), train_loss = 2.184, time/batch = 0.589\n",
      "282/2230 (epoch 0), train_loss = 2.139, time/batch = 0.570\n",
      "283/2230 (epoch 0), train_loss = 2.165, time/batch = 0.593\n",
      "284/2230 (epoch 0), train_loss = 2.163, time/batch = 0.585\n",
      "285/2230 (epoch 0), train_loss = 2.177, time/batch = 0.583\n",
      "286/2230 (epoch 0), train_loss = 2.134, time/batch = 0.595\n",
      "287/2230 (epoch 0), train_loss = 2.181, time/batch = 0.655\n",
      "288/2230 (epoch 0), train_loss = 2.194, time/batch = 0.620\n",
      "289/2230 (epoch 0), train_loss = 2.152, time/batch = 0.596\n",
      "290/2230 (epoch 0), train_loss = 2.129, time/batch = 0.599\n",
      "291/2230 (epoch 0), train_loss = 2.162, time/batch = 0.631\n",
      "292/2230 (epoch 0), train_loss = 2.104, time/batch = 0.574\n",
      "293/2230 (epoch 0), train_loss = 2.158, time/batch = 0.599\n",
      "294/2230 (epoch 0), train_loss = 2.189, time/batch = 0.577\n",
      "295/2230 (epoch 0), train_loss = 2.172, time/batch = 0.586\n",
      "296/2230 (epoch 0), train_loss = 2.206, time/batch = 0.594\n",
      "297/2230 (epoch 0), train_loss = 2.130, time/batch = 0.612\n",
      "298/2230 (epoch 0), train_loss = 2.174, time/batch = 0.606\n",
      "299/2230 (epoch 0), train_loss = 2.059, time/batch = 0.584\n",
      "300/2230 (epoch 0), train_loss = 2.096, time/batch = 0.584\n",
      "301/2230 (epoch 0), train_loss = 2.162, time/batch = 0.590\n",
      "302/2230 (epoch 0), train_loss = 2.091, time/batch = 0.571\n",
      "303/2230 (epoch 0), train_loss = 2.082, time/batch = 0.648\n",
      "304/2230 (epoch 0), train_loss = 2.163, time/batch = 0.612\n",
      "305/2230 (epoch 0), train_loss = 2.154, time/batch = 0.647\n",
      "306/2230 (epoch 0), train_loss = 2.080, time/batch = 0.603\n",
      "307/2230 (epoch 0), train_loss = 2.134, time/batch = 0.568\n",
      "308/2230 (epoch 0), train_loss = 2.177, time/batch = 0.627\n",
      "309/2230 (epoch 0), train_loss = 2.136, time/batch = 0.579\n",
      "310/2230 (epoch 0), train_loss = 2.112, time/batch = 0.572\n",
      "311/2230 (epoch 0), train_loss = 2.178, time/batch = 0.600\n",
      "312/2230 (epoch 0), train_loss = 2.140, time/batch = 0.594\n",
      "313/2230 (epoch 0), train_loss = 2.134, time/batch = 0.606\n",
      "314/2230 (epoch 0), train_loss = 2.082, time/batch = 0.580\n",
      "315/2230 (epoch 0), train_loss = 2.131, time/batch = 0.574\n",
      "316/2230 (epoch 0), train_loss = 2.136, time/batch = 0.617\n",
      "317/2230 (epoch 0), train_loss = 2.155, time/batch = 0.649\n",
      "318/2230 (epoch 0), train_loss = 2.113, time/batch = 0.639\n",
      "319/2230 (epoch 0), train_loss = 2.120, time/batch = 0.633\n",
      "320/2230 (epoch 0), train_loss = 2.097, time/batch = 0.640\n",
      "321/2230 (epoch 0), train_loss = 2.084, time/batch = 0.628\n",
      "322/2230 (epoch 0), train_loss = 2.104, time/batch = 0.612\n",
      "323/2230 (epoch 0), train_loss = 2.116, time/batch = 0.586\n",
      "324/2230 (epoch 0), train_loss = 2.115, time/batch = 0.605\n",
      "325/2230 (epoch 0), train_loss = 2.087, time/batch = 0.664\n",
      "326/2230 (epoch 0), train_loss = 2.073, time/batch = 0.622\n",
      "327/2230 (epoch 0), train_loss = 2.066, time/batch = 0.594\n",
      "328/2230 (epoch 0), train_loss = 2.128, time/batch = 0.593\n",
      "329/2230 (epoch 0), train_loss = 2.076, time/batch = 0.585\n",
      "330/2230 (epoch 0), train_loss = 2.098, time/batch = 0.631\n",
      "331/2230 (epoch 0), train_loss = 2.142, time/batch = 0.617\n",
      "332/2230 (epoch 0), train_loss = 2.104, time/batch = 0.591\n",
      "333/2230 (epoch 0), train_loss = 2.069, time/batch = 0.602\n",
      "334/2230 (epoch 0), train_loss = 2.093, time/batch = 0.586\n",
      "335/2230 (epoch 0), train_loss = 2.088, time/batch = 0.561\n",
      "336/2230 (epoch 0), train_loss = 2.076, time/batch = 0.631\n",
      "337/2230 (epoch 0), train_loss = 2.084, time/batch = 0.717\n",
      "338/2230 (epoch 0), train_loss = 2.144, time/batch = 0.587\n",
      "339/2230 (epoch 0), train_loss = 2.058, time/batch = 0.601\n",
      "340/2230 (epoch 0), train_loss = 2.104, time/batch = 0.677\n",
      "341/2230 (epoch 0), train_loss = 2.041, time/batch = 0.640\n",
      "342/2230 (epoch 0), train_loss = 2.097, time/batch = 0.618\n",
      "343/2230 (epoch 0), train_loss = 2.070, time/batch = 0.582\n",
      "344/2230 (epoch 0), train_loss = 2.033, time/batch = 0.635\n",
      "345/2230 (epoch 0), train_loss = 2.128, time/batch = 0.644\n",
      "346/2230 (epoch 0), train_loss = 2.098, time/batch = 0.565\n",
      "347/2230 (epoch 0), train_loss = 2.126, time/batch = 0.587\n",
      "348/2230 (epoch 0), train_loss = 2.064, time/batch = 0.574\n",
      "349/2230 (epoch 0), train_loss = 2.142, time/batch = 0.600\n",
      "350/2230 (epoch 0), train_loss = 2.125, time/batch = 0.587\n",
      "351/2230 (epoch 0), train_loss = 2.087, time/batch = 0.565\n",
      "352/2230 (epoch 0), train_loss = 2.066, time/batch = 0.607\n",
      "353/2230 (epoch 0), train_loss = 2.022, time/batch = 0.563\n",
      "354/2230 (epoch 0), train_loss = 2.025, time/batch = 0.584\n",
      "355/2230 (epoch 0), train_loss = 2.096, time/batch = 0.634\n",
      "356/2230 (epoch 0), train_loss = 2.086, time/batch = 0.683\n",
      "357/2230 (epoch 0), train_loss = 2.107, time/batch = 0.679\n",
      "358/2230 (epoch 0), train_loss = 2.054, time/batch = 0.717\n",
      "359/2230 (epoch 0), train_loss = 2.037, time/batch = 0.625\n",
      "360/2230 (epoch 0), train_loss = 2.071, time/batch = 0.654\n",
      "361/2230 (epoch 0), train_loss = 2.015, time/batch = 0.628\n",
      "362/2230 (epoch 0), train_loss = 2.044, time/batch = 0.629\n",
      "363/2230 (epoch 0), train_loss = 2.066, time/batch = 0.634\n",
      "364/2230 (epoch 0), train_loss = 2.061, time/batch = 0.627\n",
      "365/2230 (epoch 0), train_loss = 2.077, time/batch = 0.653\n",
      "366/2230 (epoch 0), train_loss = 2.078, time/batch = 0.644\n",
      "367/2230 (epoch 0), train_loss = 2.048, time/batch = 0.603\n",
      "368/2230 (epoch 0), train_loss = 2.072, time/batch = 0.647\n",
      "369/2230 (epoch 0), train_loss = 2.070, time/batch = 0.636\n",
      "370/2230 (epoch 0), train_loss = 2.171, time/batch = 0.629\n",
      "371/2230 (epoch 0), train_loss = 2.066, time/batch = 0.654\n",
      "372/2230 (epoch 0), train_loss = 2.071, time/batch = 0.664\n",
      "373/2230 (epoch 0), train_loss = 2.097, time/batch = 0.727\n",
      "374/2230 (epoch 0), train_loss = 2.060, time/batch = 0.695\n",
      "375/2230 (epoch 0), train_loss = 2.069, time/batch = 0.681\n",
      "376/2230 (epoch 0), train_loss = 2.070, time/batch = 0.729\n",
      "377/2230 (epoch 0), train_loss = 2.030, time/batch = 0.661\n",
      "378/2230 (epoch 0), train_loss = 2.056, time/batch = 0.590\n",
      "379/2230 (epoch 0), train_loss = 2.087, time/batch = 0.582\n",
      "380/2230 (epoch 0), train_loss = 2.058, time/batch = 0.606\n",
      "381/2230 (epoch 0), train_loss = 2.071, time/batch = 0.580\n",
      "382/2230 (epoch 0), train_loss = 2.095, time/batch = 0.581\n",
      "383/2230 (epoch 0), train_loss = 2.110, time/batch = 0.574\n",
      "384/2230 (epoch 0), train_loss = 2.048, time/batch = 0.594\n",
      "385/2230 (epoch 0), train_loss = 2.085, time/batch = 0.601\n",
      "386/2230 (epoch 0), train_loss = 2.093, time/batch = 0.614\n",
      "387/2230 (epoch 0), train_loss = 2.055, time/batch = 0.659\n",
      "388/2230 (epoch 0), train_loss = 2.099, time/batch = 0.674\n",
      "389/2230 (epoch 0), train_loss = 2.050, time/batch = 0.626\n",
      "390/2230 (epoch 0), train_loss = 2.033, time/batch = 0.645\n",
      "391/2230 (epoch 0), train_loss = 2.093, time/batch = 0.571\n",
      "392/2230 (epoch 0), train_loss = 2.084, time/batch = 0.590\n",
      "393/2230 (epoch 0), train_loss = 2.075, time/batch = 0.694\n",
      "394/2230 (epoch 0), train_loss = 2.043, time/batch = 0.631\n",
      "395/2230 (epoch 0), train_loss = 2.076, time/batch = 0.593\n",
      "396/2230 (epoch 0), train_loss = 2.024, time/batch = 0.577\n",
      "397/2230 (epoch 0), train_loss = 2.000, time/batch = 0.575\n",
      "398/2230 (epoch 0), train_loss = 2.030, time/batch = 0.593\n",
      "399/2230 (epoch 0), train_loss = 2.006, time/batch = 0.564\n",
      "400/2230 (epoch 0), train_loss = 2.077, time/batch = 0.606\n",
      "401/2230 (epoch 0), train_loss = 2.055, time/batch = 0.574\n",
      "402/2230 (epoch 0), train_loss = 2.057, time/batch = 0.570\n",
      "403/2230 (epoch 0), train_loss = 2.079, time/batch = 0.656\n",
      "404/2230 (epoch 0), train_loss = 2.053, time/batch = 0.649\n",
      "405/2230 (epoch 0), train_loss = 2.048, time/batch = 0.730\n",
      "406/2230 (epoch 0), train_loss = 2.072, time/batch = 0.671\n",
      "407/2230 (epoch 0), train_loss = 2.016, time/batch = 0.759\n",
      "408/2230 (epoch 0), train_loss = 2.107, time/batch = 0.622\n",
      "409/2230 (epoch 0), train_loss = 2.061, time/batch = 0.673\n",
      "410/2230 (epoch 0), train_loss = 2.058, time/batch = 0.629\n",
      "411/2230 (epoch 0), train_loss = 2.034, time/batch = 0.625\n",
      "412/2230 (epoch 0), train_loss = 2.046, time/batch = 0.663\n",
      "413/2230 (epoch 0), train_loss = 2.058, time/batch = 0.642\n",
      "414/2230 (epoch 0), train_loss = 2.037, time/batch = 0.667\n",
      "415/2230 (epoch 0), train_loss = 2.051, time/batch = 0.663\n",
      "416/2230 (epoch 0), train_loss = 2.047, time/batch = 0.694\n",
      "417/2230 (epoch 0), train_loss = 1.981, time/batch = 0.662\n",
      "418/2230 (epoch 0), train_loss = 2.046, time/batch = 0.713\n",
      "419/2230 (epoch 0), train_loss = 2.053, time/batch = 0.650\n",
      "420/2230 (epoch 0), train_loss = 2.033, time/batch = 0.641\n",
      "421/2230 (epoch 0), train_loss = 2.053, time/batch = 0.780\n",
      "422/2230 (epoch 0), train_loss = 2.061, time/batch = 0.685\n",
      "423/2230 (epoch 0), train_loss = 2.077, time/batch = 0.647\n",
      "424/2230 (epoch 0), train_loss = 2.053, time/batch = 0.643\n",
      "425/2230 (epoch 0), train_loss = 2.009, time/batch = 0.635\n",
      "426/2230 (epoch 0), train_loss = 2.074, time/batch = 0.645\n",
      "427/2230 (epoch 0), train_loss = 2.056, time/batch = 0.642\n",
      "428/2230 (epoch 0), train_loss = 2.038, time/batch = 0.637\n",
      "429/2230 (epoch 0), train_loss = 2.043, time/batch = 0.659\n",
      "430/2230 (epoch 0), train_loss = 2.011, time/batch = 0.671\n",
      "431/2230 (epoch 0), train_loss = 2.060, time/batch = 0.654\n",
      "432/2230 (epoch 0), train_loss = 2.027, time/batch = 0.622\n",
      "433/2230 (epoch 0), train_loss = 2.057, time/batch = 0.622\n",
      "434/2230 (epoch 0), train_loss = 2.011, time/batch = 0.607\n",
      "435/2230 (epoch 0), train_loss = 1.996, time/batch = 0.681\n",
      "436/2230 (epoch 0), train_loss = 2.055, time/batch = 0.665\n",
      "437/2230 (epoch 0), train_loss = 2.026, time/batch = 0.751\n",
      "438/2230 (epoch 0), train_loss = 2.045, time/batch = 0.650\n",
      "439/2230 (epoch 0), train_loss = 1.989, time/batch = 0.676\n",
      "440/2230 (epoch 0), train_loss = 2.019, time/batch = 0.653\n",
      "441/2230 (epoch 0), train_loss = 1.995, time/batch = 0.641\n",
      "442/2230 (epoch 0), train_loss = 1.987, time/batch = 0.777\n",
      "443/2230 (epoch 0), train_loss = 1.982, time/batch = 0.806\n",
      "444/2230 (epoch 0), train_loss = 2.001, time/batch = 0.649\n",
      "445/2230 (epoch 0), train_loss = 2.015, time/batch = 0.630\n",
      "446/2230 (epoch 1), train_loss = 2.119, time/batch = 0.605\n",
      "447/2230 (epoch 1), train_loss = 2.023, time/batch = 0.615\n",
      "448/2230 (epoch 1), train_loss = 1.987, time/batch = 0.666\n",
      "449/2230 (epoch 1), train_loss = 2.018, time/batch = 0.637\n",
      "450/2230 (epoch 1), train_loss = 2.035, time/batch = 0.590\n",
      "451/2230 (epoch 1), train_loss = 2.044, time/batch = 0.587\n",
      "452/2230 (epoch 1), train_loss = 2.063, time/batch = 0.581\n",
      "453/2230 (epoch 1), train_loss = 2.051, time/batch = 0.627\n",
      "454/2230 (epoch 1), train_loss = 1.985, time/batch = 0.627\n",
      "455/2230 (epoch 1), train_loss = 1.934, time/batch = 0.661\n",
      "456/2230 (epoch 1), train_loss = 2.038, time/batch = 0.626\n",
      "457/2230 (epoch 1), train_loss = 2.000, time/batch = 0.569\n",
      "458/2230 (epoch 1), train_loss = 2.007, time/batch = 0.636\n",
      "459/2230 (epoch 1), train_loss = 1.969, time/batch = 0.632\n",
      "460/2230 (epoch 1), train_loss = 1.965, time/batch = 0.615\n",
      "461/2230 (epoch 1), train_loss = 1.956, time/batch = 0.601\n",
      "462/2230 (epoch 1), train_loss = 2.037, time/batch = 0.584\n",
      "463/2230 (epoch 1), train_loss = 1.977, time/batch = 0.590\n",
      "464/2230 (epoch 1), train_loss = 1.986, time/batch = 0.582\n",
      "465/2230 (epoch 1), train_loss = 1.995, time/batch = 0.580\n",
      "466/2230 (epoch 1), train_loss = 1.960, time/batch = 0.586\n",
      "467/2230 (epoch 1), train_loss = 1.974, time/batch = 0.571\n",
      "468/2230 (epoch 1), train_loss = 1.971, time/batch = 0.595\n",
      "469/2230 (epoch 1), train_loss = 1.985, time/batch = 0.589\n",
      "470/2230 (epoch 1), train_loss = 1.950, time/batch = 0.578\n",
      "471/2230 (epoch 1), train_loss = 1.992, time/batch = 0.595\n",
      "472/2230 (epoch 1), train_loss = 1.931, time/batch = 0.569\n",
      "473/2230 (epoch 1), train_loss = 1.959, time/batch = 0.586\n",
      "474/2230 (epoch 1), train_loss = 1.979, time/batch = 0.599\n",
      "475/2230 (epoch 1), train_loss = 1.986, time/batch = 0.580\n",
      "476/2230 (epoch 1), train_loss = 1.981, time/batch = 0.585\n",
      "477/2230 (epoch 1), train_loss = 2.023, time/batch = 0.589\n",
      "478/2230 (epoch 1), train_loss = 1.939, time/batch = 0.597\n",
      "479/2230 (epoch 1), train_loss = 1.963, time/batch = 0.679\n",
      "480/2230 (epoch 1), train_loss = 1.991, time/batch = 0.690\n",
      "481/2230 (epoch 1), train_loss = 1.973, time/batch = 0.640\n",
      "482/2230 (epoch 1), train_loss = 2.073, time/batch = 0.586\n",
      "483/2230 (epoch 1), train_loss = 2.027, time/batch = 0.584\n",
      "484/2230 (epoch 1), train_loss = 2.006, time/batch = 0.596\n",
      "485/2230 (epoch 1), train_loss = 2.007, time/batch = 0.587\n",
      "486/2230 (epoch 1), train_loss = 2.031, time/batch = 0.582\n",
      "487/2230 (epoch 1), train_loss = 1.993, time/batch = 0.570\n",
      "488/2230 (epoch 1), train_loss = 1.973, time/batch = 0.603\n",
      "489/2230 (epoch 1), train_loss = 1.950, time/batch = 0.586\n",
      "490/2230 (epoch 1), train_loss = 1.994, time/batch = 0.622\n",
      "491/2230 (epoch 1), train_loss = 1.970, time/batch = 0.803\n",
      "492/2230 (epoch 1), train_loss = 1.939, time/batch = 0.647\n",
      "493/2230 (epoch 1), train_loss = 1.958, time/batch = 0.661\n",
      "494/2230 (epoch 1), train_loss = 1.957, time/batch = 0.627\n",
      "495/2230 (epoch 1), train_loss = 1.932, time/batch = 0.624\n",
      "496/2230 (epoch 1), train_loss = 1.961, time/batch = 0.620\n",
      "497/2230 (epoch 1), train_loss = 1.924, time/batch = 0.607\n",
      "498/2230 (epoch 1), train_loss = 1.931, time/batch = 0.577\n",
      "499/2230 (epoch 1), train_loss = 1.988, time/batch = 0.577\n",
      "500/2230 (epoch 1), train_loss = 1.975, time/batch = 0.580\n",
      "model saved to save/model.ckpt\n",
      "501/2230 (epoch 1), train_loss = 1.950, time/batch = 0.610\n",
      "502/2230 (epoch 1), train_loss = 1.938, time/batch = 0.665\n",
      "503/2230 (epoch 1), train_loss = 1.971, time/batch = 0.606\n",
      "504/2230 (epoch 1), train_loss = 1.960, time/batch = 0.629\n",
      "505/2230 (epoch 1), train_loss = 1.951, time/batch = 0.603\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d46154699ebd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-315d40d1ea0c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m                 \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\"                     .format(e * data_loader.num_batches + b,\n",
      "\u001b[0;32m/Users/damienhenry/code/jupyter/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 340\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    341\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/damienhenry/code/jupyter/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 564\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    565\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m       \u001b[0;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/damienhenry/code/jupyter/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 637\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    638\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/damienhenry/code/jupyter/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    642\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m       \u001b[0merror_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/damienhenry/code/jupyter/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m--> 628\u001b[0;31m             session, None, feed_dict, fetch_list, target_list, None)\n\u001b[0m\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess =  tf.Session()\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_checkpoint_path: \"save/model.ckpt-500\"\n",
      "all_model_checkpoint_paths: \"save/model.ckpt-0\"\n",
      "all_model_checkpoint_paths: \"save/model.ckpt-500\"\n",
      "\n",
      " yhe arpieses? This geigsteld will ony of the me fued's frill to wishs that have omall'd besy.\n",
      "Wo sselp swarch\n",
      "innecinf, yean laceryister, aghe I gay I poke son..\n",
      "\n",
      "IUNTHAEL:\n",
      "Ore my so the bumie klroter our seelloln mother a waroe\n",
      "To thoughar ad me, I, carchy hoon croldt hive beek the a kuth proth me atace as my wore with you old the miver,\n",
      "Buthan,\n",
      "Withe, I by, string of the stidar\n",
      "Rof.\n",
      "Geting ix in herch bectar I gorll mage in watet mird: I love you hear busho Irre cive, him and suck thesfores.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sample(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedback wellcome @dh7net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
