{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Character-level TensorFlow RNN model. \n",
    "\n",
    "The following code is an adaptation to TensorFlow of the _[Minimal character-level Vanilla RNN model](https://gist.github.com/karpathy/d4dee566867f8291f086)_ written by __Andrej Karpathy__.  \n",
    "\n",
    "[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a great source of inspiration to understand the power of RNN.\n",
    "\n",
    "This notebook is for beginners who whant to understand RNN and the basis of TensorFlow by reading code.\n",
    "\n",
    "More ressources:\n",
    "* [Minimal character-level Vanilla RNN model explained in a notebook](https://github.com/dh7/ML-Tutorial-Notebooks/blob/master/RNN.ipynb)\n",
    "* [RNN in TensorFlow explained in a notebook](https://github.com/dh7/ML-Tutorial-Notebooks/blob/master/tf-char-RNN.ipynb)\n",
    "* [A model that implement LSTM with 2 layer](https://github.com/sherjilozair/char-rnn-tensorflow) from __Sherjil Ozair__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Minimal character-level TensorFlow RNN model.\n",
    "Original code written by Andrej Karpathy (@karpathy),  \n",
    "adapted to TensorFlow by Damien Henry (@dh7net)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph() # Useful in Jupyter, to run the code several times\n",
    "\n",
    "# data I/O\n",
    "data = open('methamorphosis.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print 'data has %d characters, %d unique.' % (data_size, vocab_size)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) } # to convert a char to an ID\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) } # to convert an ID back to a char\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 0.002\n",
    "decay_rate = 0.98 # \n",
    "\n",
    "# model parameters\n",
    "Wxh = tf.Variable(tf.random_uniform((hidden_size, vocab_size))*0.01, name='Wxh') #input to hidden\n",
    "Whh = tf.Variable(tf.random_uniform((hidden_size, hidden_size))*0.01, name='Whh')#hidden to hidden\n",
    "Why = tf.Variable(tf.random_uniform((vocab_size, hidden_size))*0.01, name='Why') #hidden to output\n",
    "bh = tf.Variable(tf.zeros((hidden_size, 1)), name='bh') # hidden bias\n",
    "by = tf.Variable(tf.zeros((vocab_size, 1)), name='by') # output bias\n",
    "\n",
    "# Define placeholder to for the input and the target & create the sequences\n",
    "input_data = tf.placeholder(tf.float32, [seq_length, vocab_size], name='input_data')\n",
    "xs = tf.split(0, seq_length, input_data)\n",
    "target_data = tf.placeholder(tf.float32, [seq_length, vocab_size], name='target_data') \n",
    "targets = tf.split(0, seq_length, target_data)  \n",
    "# initial_state & loss\n",
    "initial_state = tf.zeros((hidden_size, 1))\n",
    "loss = tf.zeros([1], name='loss')\n",
    "# unroll recursion to create the forward pass graph\n",
    "hs, ys, ps = {}, {}, {}\n",
    "hs[-1] = initial_state                                                                                                                                                                             \n",
    "for t in xrange(seq_length):\n",
    "    xs_t = tf.transpose(xs[t])\n",
    "    targets_t = tf.transpose(targets[t]) \n",
    "    hs[t] = tf.tanh(tf.matmul(Wxh, xs_t) + tf.matmul(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = tf.matmul(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = tf.exp(ys[t]) / tf.reduce_sum(tf.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -tf.log(tf.reduce_sum(tf.mul(ps[t], targets_t))) # softmax (cross-entropy loss)\n",
    "\n",
    "cost = loss / seq_length\n",
    "final_state = hs[seq_length-1]\n",
    "lr = tf.Variable(0.0, trainable=False, name='learning_rate')\n",
    "tvars = tf.trainable_variables()\n",
    "# Calculation of gradient is done by TensorFlow using \"tf.gradients(cost, tvars)\"\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), 5) # clip exploding gradients\n",
    "optimizer = tf.train.AdamOptimizer(lr) \n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in xrange(n):\n",
    "    h = np.tanh(np.dot(Wxh.eval(), x) + np.dot(Whh.eval(), h) + bh.eval())\n",
    "    y = np.dot(Why.eval(), h) + by.eval()\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "def vectorize(x): # take an array of IX and return an array of vector\n",
    "    vectorized = np.zeros((len(x), vocab_size))\n",
    "    for i in range(0, len(x)):\n",
    "        vectorized[i][x[i]] = 1\n",
    "    return vectorized\n",
    "\n",
    "n, p, epoch = 0, 0, 0\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print \"all variable initialized\"\n",
    "    while True:\n",
    "        # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "        if p+seq_length+1 >= len(data) or n == 0: \n",
    "            state = initial_state.eval() # reset RNN memory\n",
    "            sess.run(tf.assign(lr, learning_rate * (decay_rate ** epoch)))\n",
    "            p = 0 # go from start of data\n",
    "            epoch += 1 # increase epoch number\n",
    "        x = vectorize([char_to_ix[ch] for ch in data[p:p+seq_length]])\n",
    "        y = vectorize([char_to_ix[ch] for ch in data[p+1:p+seq_length+1]])\n",
    "        # Create the structure for the learning data\n",
    "        feed = {input_data: x, target_data: y, initial_state: state}\n",
    "        # Run a session using train_op\n",
    "        [train_loss], state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "        smooth_loss = smooth_loss * 0.999 + train_loss * 0.001\n",
    "        # sample from the model now and then\n",
    "        if n % 1000 == 0:\n",
    "            print 'iter %d, loss: %f' % (n, smooth_loss) # print progress\n",
    "            sample_ix = sample(state, char_to_ix[data[p]], 200)\n",
    "            txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "            print '----\\n %s \\n----' % (txt, )\n",
    "\n",
    "        p += seq_length # move data pointer\n",
    "        n += 1 # iteration counter \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedback welcome __@dh7net__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
