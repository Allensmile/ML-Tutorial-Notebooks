{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal character-level Vanilla RNN model.\n",
    "\n",
    "RNN stand for \"Recurent Neural Network\".  \n",
    "To understand why RNN are so hot you _must_ read [this](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)!  \n",
    "\n",
    "This notebook to explain the _[Minimal character-level Vanilla RNN model](https://gist.github.com/karpathy/d4dee566867f8291f086)_ written by __Andrej Karpathy__  \n",
    "This code create a RNN to generate a text, char after char, by learning char after char from a textfile.\n",
    "\n",
    "I love this _character-level Vanilla RNN_ code because it doesn't use any library except numpy.\n",
    "All the NN magic in 112 lines of code, no need to understand any dependency. Everything is there! I'll try to explain in detail every line of it.\n",
    "\n",
    "This notebook is for real beginners who whant to understand RNN concept by reading code.  \n",
    "Feedback welcome __@dh7net__\n",
    " \n",
    "## Let's start!  \n",
    "First let's run the whole code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 119163 characters, 61 unique.\n",
      "----\n",
      " xrvvvQSiTpL,FvEnf,Ipcgj(EEouhUDNY AdH(t'elMgwgHye)yjOyWouEfl\n",
      "uIhbf:i,vPSl(P:sw\n",
      "ka,wfxaFSHDtJnj:ez;NIiv\"FYu;hegJ;PSGFfehrpjt''w;Vw(LMo\"\"DGcqcJVP\"nAeMqO;dVDyEBblVM\"xbOn\n",
      "Lr-lmzuM\n",
      "!PFlJsEOwlDqQNGrcQfw'nms \n",
      "----\n",
      "iter 0, loss: 102.771834\n",
      "----\n",
      " asaarus nss afdey o\n",
      "a  o ostoaaen oohieel tsb es esass  r aseassua see seaofaa,ensausoaausu s o aemohshgaoou ssoefoa asdseohastate.usy   wna ieeip  oose  tooe su  oss c    darenavtsed  b soobspis wnge \n",
      "----\n",
      "iter 100, loss: 102.568394\n",
      "----\n",
      " .hatd neooivtw mwne g  ddsevbr ibe revnnhus.oGe si- t nrt\n",
      "l vcb  dssheoo  eee nun yes uomdro aubfcywf sku wh g btsan bia akte slrkt b s  snsodobh d To yi o s o o c dod\n",
      "fb ybaute bewonwsesu nilx   oi n \n",
      "----\n",
      "iter 200, loss: 100.476990\n",
      "----\n",
      "  ve  anos g n oba  e e are ai o oet e ge   to be m fe gab e  dipot eo.e unhia\n",
      " not  d ne  a  it uag  a ghkce' ut n wert a ae t,iou aneoiw  pilhtg weu ! ant ele m eeea, t  ig  \n",
      "eice teecee se teeigiven \n",
      "----\n",
      "iter 300, loss: 98.186100\n",
      "----\n",
      " nonsetosrt sresdnnbs n\n",
      "isn vtc.os tsnhq namlds snawstswin bnn heribunsnn\n",
      "mnett censsoutoss oft\n",
      "cnuslscs cis noucs scanbisnnta\n",
      "eolsncsnkttuacerisassnswonyinntstlt t.osn el caclostly csre tostessarcnsrm \n",
      "----\n",
      "iter 400, loss: 95.410661\n",
      "----\n",
      " tr ele fe\n",
      " fotfee beils bese dheweld, go d fle le tugres t  e thelg\n",
      "tenlc, tt l.sl\n",
      "tle woulevaweild f mluilf todt, w eno,rillodlere ul  thea pint  re lf lo,le ddey oat musn uolu elmo le nal aretanfoem \n",
      "----\n",
      "iter 500, loss: 92.636035\n",
      "----\n",
      " toG'wralfoire ho ponschi tffinf thinshimt paneklisg ane this pospiishgltiself to thisiklgsiry hesitiimbor'm\n",
      "ir,epot hiiphimt helr this pathik pt himnis whe his hinripsios mtrk\n",
      "khes'on ths hanpingithag \n",
      "----\n",
      "iter 600, loss: 90.016469\n",
      "----\n",
      " athensk \n",
      "emd dhe\n",
      " ove bhe th?csd t eeede t, wer t t yacs hesen tote\".nd west  pos toctjente  go oow an th hle dlus he soegad\n",
      "sagesgheisecanghecrat aalp.;, pt an? pd dedsr lle wnes\n",
      "Gn. cat cand  ht has \n",
      "----\n",
      "iter 700, loss: 87.859229\n",
      "----\n",
      " emt axt\n",
      "waim omille\n",
      "i  at erim euldeeimeo\"lg, pm Ify tomirif\n",
      "mamnrothep halaly thinshepe monttaf tomtqlegly hin, ba te onlegathe. mhenl  fe ,,ienr nonl homIceut,ot sar hrntln yIcehangFawamope toull wa \n",
      "----\n",
      "iter 800, loss: 85.761566\n",
      "----\n",
      " ovle!fher \"he \"hiweyiwlere chegh.  mo.\n",
      "pt mh! the\n",
      "afane firl!\n",
      "cee\"y sewtG\"\n",
      "chirinfei!, rees n!ole\"f!ku\" the,incle l\" lis he thed \n",
      " alletefefe !h. nnm ggele trif\"aicielfes!relk ru\"sd hh chise tid he gy \n",
      "----\n",
      "iter 900, loss: 83.756340\n",
      "----\n",
      " teuit rostorek pacut rnlw nrwius wG , he t sme fet ve as hamwre bi le se tt thet\n",
      "wuse heve- wousn-t, hisgthi onem the be seny thekns we odunang to torse wher he had wesfiu s whaug tha, tian nt\n",
      "avy the \n",
      "----\n",
      "iter 1000, loss: 81.741296\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('methamorphosis.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print 'data has %d characters, %d unique.' % (data_size, vocab_size)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in xrange(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(xrange(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in xrange(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while n<=1000: # was while True: in original code\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print '----\\n %s \\n----' % (txt, )\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: print 'iter %d, loss: %f' % (n, smooth_loss) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not a NN expert, the code is not easy to understand.  \n",
    "\n",
    "If you look to the results you can see that the code iterate 1000 time, calculate a __loss__ that decrease over time, and output some text each 100 itaration.\n",
    "The output from the first iteration looks random.\n",
    "After 1000 iterations, the NN is able to create words that have plausible size, don't use too much caps, and can create correct small words like \"the\", \"they\", \"be\", \"to\".\n",
    "If you let the code learn over a nigth the NN will be able to create correct sentences. \n",
    "\n",
    "## Theorie\n",
    "This code build a neural network that is able to predict a char from the previous one.  \n",
    "It learn from a text file, so he can learn words and sentence.  \n",
    "At each step it can use some results from the previous step to keep in memory what is going on.  \n",
    "For instance if the previous char are \"hello worl\" the model can guess that the next char is \"d\".\n",
    "\n",
    "This model contain parameters that are initialized randomly and the trainning phase try to find optimal values for each of them. \n",
    "During the trainning process we do a gradient descent:\n",
    "* We give to the model a pair of char: the input char and the target char. The target char is the char the network should guess, it is the next char in our trainning text file.\n",
    "* We calculate the probability for every possible next char according to the state of the model, using the paramters (This is the forward pass).\n",
    "* We create a distance (the loss) between the previous probabilty and the target char.\n",
    "* We calculate gradients for each of our parameters to see witch impact they have on the loss. (A fast way to calculate all gradients is called the backward pass).\n",
    "* We update all parameters in the direction that help to minimise the loss\n",
    "* We iterate until their is no more progress\n",
    "\n",
    "# Let's dive in! \n",
    "\n",
    "## The code contains 4 parts\n",
    "* Load the trainning data\n",
    "  * and encode decode char into vectors\n",
    "* Define the Network\n",
    "* Define a function to create sentences from the model\n",
    "* Define a loss function\n",
    "  * Forward pass\n",
    "  * Loss\n",
    "  * Backward pass\n",
    "* Train the network\n",
    "  * Feed the network\n",
    "  * Calculate gradiend and use Adagrad to update the model parameters\n",
    "  * Output a text to see the progress of the training\n",
    " \n",
    "Let's have a closer look to every line of the code.\n",
    "\n",
    "## Load the training data\n",
    "\n",
    "The network need a big txt file as an input.\n",
    "\n",
    "The content of the file will be used to train the network.\n",
    "\n",
    "For this example, I used Methamorphosis from Kafka (Public Domain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"                                                                                                                                                                                           \n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)                                                                                                             \n",
    "BSD License                                                                                                                                                                                   \n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O                                                                                                                                                                                    \n",
    "data = open('methamorphosis.txt', 'r').read() # should be simple plain text file   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode/Decode char/vector\n",
    "Neural networks can only works on vectors. (a vector is an array of float)\n",
    "So we need a way to encode and decode a char as a vector.\n",
    "\n",
    "For this we count the number of unique char (*vocab_size*). It will be the size of the vector. \n",
    "The vector contain only zero exept for the position of the char wherae the value is 1.\n",
    "\n",
    "#### First we calculate *vocab_size*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 119163 characters, 61 unique.\n"
     ]
    }
   ],
   "source": [
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print 'data has %d characters, %d unique.' % (data_size, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we create 2 dictionary to encode and decode a char to an int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, '!': 1, ' ': 2, '\"': 3, \"'\": 4, ')': 5, '(': 6, '-': 7, ',': 8, '.': 9, ';': 10, ':': 11, '?': 12, 'A': 13, 'C': 14, 'B': 15, 'E': 16, 'D': 17, 'G': 18, 'F': 19, 'I': 20, 'H': 21, 'J': 22, 'M': 23, 'L': 24, 'O': 25, 'N': 26, 'Q': 27, 'P': 28, 'S': 29, 'U': 30, 'T': 31, 'W': 32, 'V': 33, 'Y': 34, 'a': 35, 'c': 36, 'b': 37, 'e': 38, 'd': 39, 'g': 40, 'f': 41, 'i': 42, 'h': 43, 'k': 44, 'j': 45, 'm': 46, 'l': 47, 'o': 48, 'n': 49, 'q': 50, 'p': 51, 's': 52, 'r': 53, 'u': 54, 't': 55, 'w': 56, 'v': 57, 'y': 58, 'x': 59, 'z': 60}\n",
      "{0: '\\n', 1: '!', 2: ' ', 3: '\"', 4: \"'\", 5: ')', 6: '(', 7: '-', 8: ',', 9: '.', 10: ';', 11: ':', 12: '?', 13: 'A', 14: 'C', 15: 'B', 16: 'E', 17: 'D', 18: 'G', 19: 'F', 20: 'I', 21: 'H', 22: 'J', 23: 'M', 24: 'L', 25: 'O', 26: 'N', 27: 'Q', 28: 'P', 29: 'S', 30: 'U', 31: 'T', 32: 'W', 33: 'V', 34: 'Y', 35: 'a', 36: 'c', 37: 'b', 38: 'e', 39: 'd', 40: 'g', 41: 'f', 42: 'i', 43: 'h', 44: 'k', 45: 'j', 46: 'm', 47: 'l', 48: 'o', 49: 'n', 50: 'q', 51: 'p', 52: 's', 53: 'r', 54: 'u', 55: 't', 56: 'w', 57: 'v', 58: 'y', 59: 'x', 60: 'z'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "print char_to_ix\n",
    "print ix_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finaly we create a vector from a char like this:\n",
    "The dictionary defined above allow us to create a vector of size 61 instead of 256.  \n",
    "Here and exemple for char 'a'  \n",
    "The vector contains only zero, except at position char_to_ix['a'] where we put a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "vector_for_char_a = np.zeros((vocab_size, 1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "#print vector_for_char_a\n",
    "print vector_for_char_a.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the network\n",
    "\n",
    "The neural network is made of 3 layers:\n",
    "* an input layer\n",
    "* an hidden layer\n",
    "* an output layer\n",
    "\n",
    "All layers are fully connected.  \n",
    "The hidden layer is connected to the output and to itself: the values from an iteration are used for the next one.\n",
    "\n",
    "To centralise values that matter for the training (_hyper parameters_) we also define the _sequence lenght_ and the _learning rate_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hyperparameters                                                                                                                                                                             \n",
    "hidden_size = 100 # size of hidden layer of neurons                                                                                                                                           \n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wxh contain 6100 parameters\n",
      "Whh contain 10000 parameters\n",
      "Why contain 6100 parameters\n",
      "bh contain 100 parameters\n",
      "by contain 61 parameters\n"
     ]
    }
   ],
   "source": [
    "# model parameters                                                                                                                                                                            \n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "print 'Wxh contain', Wxh.size, 'parameters'\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "print 'Whh contain', Whh.size, 'parameters'\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output    \n",
    "print 'Why contain', Why.size, 'parameters'\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "print 'bh contain', bh.size, 'parameters'\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "print 'by contain', by.size, 'parameters'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model parameters are adjusted during the trainning.\n",
    "* _Wxh_ are parameters to connect a vector that contain one input to the hidden layer.\n",
    "* _Whh_ are parameters to connect the hidden layer to itself. This is the Key of the Rnn: Recursion is done by injecting the previous values from the output of the hidden state, to itself at the next iteration.\n",
    "* _Why_ are parameters to connect the hidden layer to the output\n",
    "* _bh_ contains the hidden bias\n",
    "* _by_ contains the output bias\n",
    "\n",
    "You'll see in the next section how theses parameters are used to create a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a sentence from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " (t\"?rBYcLzsnzMyNAv cWbEn?a'HaGtP:dBicF\n",
      "zypJWcgCLkYICqlUBgUVafimymwYH(\n",
      "bwCbC\"Fm,cceBWk hyr\"!kQmPBum!kpxE:'LtdTPjxEjfduN-\"kQuFWU.BAaD,hPADtfLx) jO!nLJl,vk(dDbx;I\n",
      "(,mmBhnzQaHbMabhB:(FPv?drB-WodnMY,vMhe'w \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "  \"\"\"                                                                                                                                                                                         \n",
    "  sample a sequence of integers from the model                                                                                                                                                \n",
    "  h is memory state, seed_ix is seed letter for first time step                                                                                                                               \n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in xrange(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "  print '----\\n %s \\n----' % (txt, )\n",
    "\n",
    "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "sample(hprev,char_to_ix['a'],200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function\n",
    "The __loss__ is a key concept.  \n",
    "It is a value that describe how bag/good is our model.  \n",
    "It is always positive, the closest to zero, the better is our model.  \n",
    "(A good model is a model where the predicted output is close to the training output)\n",
    "  \n",
    "During the trainning phase we want to minimize the loss.\n",
    "\n",
    "The loss function calculate the loss but also the gradients (see backward pass):\n",
    "* It perform a forward pass to calculate the next char given a char from the trainning set.\n",
    "* It calculate the loss by comparing the predicted char to the following char of the tranning set.\n",
    "* It calculate the backward pass to calculate the gradients\n",
    "\n",
    "This function take as input:\n",
    "* a list of input char\n",
    "* a list of target char\n",
    "* and the previous hidden state\n",
    "\n",
    "This function output:\n",
    "* the loss\n",
    "* the gradient for each parameters between layers\n",
    "* the last hidden state\n",
    "\n",
    "Here the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"                                                                                                                                                                                         \n",
    "  inputs,targets are both list of integers.                                                                                                                                                   \n",
    "  hprev is Hx1 array of initial hidden state                                                                                                                                                  \n",
    "  returns the loss, gradients on model parameters, and last hidden state                                                                                                                      \n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass                                                                                                                                                                              \n",
    "  for t in xrange(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation                                                                                                                        \n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state                                                                                                            \n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars                                                                                                           \n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars                                                                                                              \n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)                                                                                                                       \n",
    "  # backward pass: compute gradients going backwards                                                                                                                                          \n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(xrange(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y                                                                                                                                                     \n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "The forward pass use the parameters of the model (Wxh, Whh, Why, bh, by) to calculate the next char given a char from the trainning set.\n",
    "\n",
    "xs[t] is the vector that encode the char at position t\n",
    "ps[t] is the probabilities for next char\n",
    "\n",
    "```python\n",
    "hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "```\n",
    "\n",
    "or is dirty pseudo code for each char\n",
    "```python\n",
    "hs = input*Wxh + last_value_of_hidden_state*Whh + bh\n",
    "ys = hs*Why + by\n",
    "ps = normalized(ys)\n",
    "```\n",
    "\n",
    "To dive into the code, we'll work on one char only (we set t=0 ; instead of the for each)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01651179  0.01642741  0.01629968  0.01645583  0.01644896  0.01625604\n",
      "  0.01623776  0.01654136  0.01644857  0.01626252  0.01596373  0.01613876\n",
      "  0.01675783  0.01614578  0.01673236  0.01665156  0.01640061  0.01628567\n",
      "  0.01613562  0.01648003  0.01602888  0.01634054  0.01661922  0.01652779\n",
      "  0.01631034  0.01597116  0.01649952  0.01634823  0.0165061   0.01659232\n",
      "  0.01663144  0.01658593  0.01651304  0.01639281  0.01646073  0.01651658\n",
      "  0.0164507   0.01638602  0.01622293  0.01659311  0.0164573   0.01637815\n",
      "  0.01633373  0.01640152  0.01665601  0.01648789  0.01649384  0.01608145\n",
      "  0.01625795  0.01657984  0.01641883  0.0163816   0.01622493  0.01644234\n",
      "  0.01641011  0.01624322  0.01615029  0.01623067  0.01612011  0.0167561\n",
      "  0.0164149 ]\n"
     ]
    }
   ],
   "source": [
    "# uncomment the print to get some details\n",
    "xs, hs, ys, ps = {}, {}, {}, {}\n",
    "hs[-1] = np.copy(hprev)\n",
    "# forward pass                                                                                                                                                                              \n",
    "t=0 # for t in xrange(len(inputs)):\n",
    "xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "xs[t][inputs[t]] = 1 \n",
    "# print xs[t]\n",
    "hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state \n",
    "ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "# print ys[t]\n",
    "ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars  \n",
    "print ps[t].ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the previous code several time. A probability is generated during each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(\n",
      ")= 0.0165 \n",
      "p(!)= 0.0164  p( )= 0.0163  p(\")= 0.0165  p(')= 0.0164  p())= 0.0163  p(()= 0.0162  p(-)= 0.0165 \n",
      "p(,)= 0.0164  p(.)= 0.0163  p(;)= 0.0160  p(:)= 0.0161  p(?)= 0.0168  p(A)= 0.0161  p(C)= 0.0167 \n",
      "p(B)= 0.0167  p(E)= 0.0164  p(D)= 0.0163  p(G)= 0.0161  p(F)= 0.0165  p(I)= 0.0160  p(H)= 0.0163 \n",
      "p(J)= 0.0166  p(M)= 0.0165  p(L)= 0.0163  p(O)= 0.0160  p(N)= 0.0165  p(Q)= 0.0163  p(P)= 0.0165 \n",
      "p(S)= 0.0166  p(U)= 0.0166  p(T)= 0.0166  p(W)= 0.0165  p(V)= 0.0164  p(Y)= 0.0165  p(a)= 0.0165 \n",
      "p(c)= 0.0165  p(b)= 0.0164  p(e)= 0.0162  p(d)= 0.0166  p(g)= 0.0165  p(f)= 0.0164  p(i)= 0.0163 \n",
      "p(h)= 0.0164  p(k)= 0.0167  p(j)= 0.0165  p(m)= 0.0165  p(l)= 0.0161  p(o)= 0.0163  p(n)= 0.0166 \n",
      "p(q)= 0.0164  p(p)= 0.0164  p(s)= 0.0162  p(r)= 0.0164  p(u)= 0.0164  p(t)= 0.0162  p(w)= 0.0162 \n",
      "p(v)= 0.0162  p(y)= 0.0161  p(x)= 0.0168  p(z)= 0.0164 \n"
     ]
    }
   ],
   "source": [
    "# Let's build a dict to see witch probablity is associated with witch char\n",
    "probability_per_char =  { ch:ps[t].ravel()[i] for i,ch in enumerate(chars) }\n",
    "# uncoment the next line to see the raw result\n",
    "# print probability_per_char\n",
    "\n",
    "# To print the probability in a way that is more easy to read.\n",
    "for x in range(vocab_size):\n",
    "    print 'p(' + ix_to_char[x] + \")=\", \"%.4f\" % ps[t].ravel()[x],\n",
    "    if (x%7==0):\n",
    "        print \"\"\n",
    "    else:\n",
    "        print \"\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next char is : D\n"
     ]
    }
   ],
   "source": [
    "# We can create the next char from the above distribution\n",
    "ix = np.random.choice(range(vocab_size), p=ps[t].ravel())\n",
    "print \"Next char is :\", ix_to_char[ix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But during the trainning process, we don't want to calculate char,  \n",
    "We want to calculate a loss to mesure how good/bad our model is.\n",
    "\n",
    "### Loss\n",
    "For each char in the input the forward pass calculate the probability of the next char  \n",
    "The loss is the sum \n",
    "```python\n",
    "loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "```\n",
    "\n",
    "The loss is calculate using Softmax. [more info here](https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/) and [here](https://en.wikipedia.org/wiki/Softmax_function).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next char from training (target) was number 36 witch is \"c\"\n",
      "Probability for this letter was 0.0164507007455\n",
      "loss for this input&target pair is 4.10738720417\n"
     ]
    }
   ],
   "source": [
    "print 'Next char from training (target) was number', targets[t], 'witch is \"' + ix_to_char[targets[t]] + '\"'\n",
    "print 'Probability for this letter was', ps[t][targets[t],0]\n",
    "\n",
    "loss = -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "print 'loss for this input&target pair is', loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Backward pass\n",
    "\n",
    "The naive way to calculate all gradients would be to recalculate a loss for small variations for each parameters.\n",
    "This is possible but super time consuming.\n",
    "There is a technics to calculates all the gradients for all the parameters at once, the backdrop propagation.  \n",
    "Gradients are calculated in the oposite order of the forward pass, using simple technics.\n",
    "[Here](http://karpathy.github.io/neuralnets/) a great source to understand this technic in detail.\n",
    "\n",
    "#### goal is to calculate gradients for the forward formula:\n",
    "hs = input*Wxh + last_value_of_hidden_state*Whh + bh  \n",
    "ys = hs*Why + by\n",
    "\n",
    "dby = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01651179  0.01642741  0.01629968  0.01645583  0.01644896  0.01625604\n",
      "  0.01623776  0.01654136  0.01644857  0.01626252  0.01596373  0.01613876\n",
      "  0.01675783  0.01614578  0.01673236  0.01665156  0.01640061  0.01628567\n",
      "  0.01613562  0.01648003  0.01602888  0.01634054  0.01661922  0.01652779\n",
      "  0.01631034  0.01597116  0.01649952  0.01634823  0.0165061   0.01659232\n",
      "  0.01663144  0.01658593  0.01651304  0.01639281  0.01646073  0.01651658\n",
      " -0.9835493   0.01638602  0.01622293  0.01659311  0.0164573   0.01637815\n",
      "  0.01633373  0.01640152  0.01665601  0.01648789  0.01649384  0.01608145\n",
      "  0.01625795  0.01657984  0.01641883  0.0163816   0.01622493  0.01644234\n",
      "  0.01641011  0.01624322  0.01615029  0.01623067  0.01612011  0.0167561\n",
      "  0.0164149 ]\n",
      "[ 0.00054093 -0.00129342  0.00028735 ...,  0.00041503 -0.00423059\n",
      "  0.00015588]\n",
      "[ 0.01651179  0.01642741  0.01629968  0.01645583  0.01644896  0.01625604\n",
      "  0.01623776  0.01654136  0.01644857  0.01626252  0.01596373  0.01613876\n",
      "  0.01675783  0.01614578  0.01673236  0.01665156  0.01640061  0.01628567\n",
      "  0.01613562  0.01648003  0.01602888  0.01634054  0.01661922  0.01652779\n",
      "  0.01631034  0.01597116  0.01649952  0.01634823  0.0165061   0.01659232\n",
      "  0.01663144  0.01658593  0.01651304  0.01639281  0.01646073  0.01651658\n",
      " -0.9835493   0.01638602  0.01622293  0.01659311  0.0164573   0.01637815\n",
      "  0.01633373  0.01640152  0.01665601  0.01648789  0.01649384  0.01608145\n",
      "  0.01625795  0.01657984  0.01641883  0.0163816   0.01622493  0.01644234\n",
      "  0.01641011  0.01624322  0.01615029  0.01623067  0.01612011  0.0167561\n",
      "  0.0164149 ]\n"
     ]
    }
   ],
   "source": [
    "# backward pass: compute gradients going backwards                                                                                                                                          \n",
    "dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "dhnext = np.zeros_like(hs[0])\n",
    "t=0 #for t in reversed(xrange(len(inputs))):\n",
    "dy = np.copy(ps[t])\n",
    "dy[targets[t]] -= 1 # backprop into y   \n",
    "print dy.ravel()\n",
    "dWhy += np.dot(dy, hs[t].T)\n",
    "print dWhy.ravel()\n",
    "dby += dy\n",
    "print dby.ravel()\n",
    "dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "dbh += dhraw\n",
    "dWxh += np.dot(dhraw, xs[t].T)\n",
    "dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "dhnext = np.dot(Whh.T, dhraw)\n",
    "for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "  np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In the main loop we will:\n",
    "* Feed the network with portion of the file. Size of cunck is *seq_lengh*\n",
    "* Use the loss function to:\n",
    "  * Do forward pass to calculate all parameters for the model for a given input/output pairs\n",
    "  * Do backward pass to calculate all gradiens\n",
    "* Print a sentence from a random seed using the parameters of the network\n",
    "* Update the model using the Adaptative Gradien technique Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Feed the loss function with inputs and targets\n",
    "\n",
    "We create two array of char from the data file,\n",
    "the targets one is shifted compare to the inputs one.\n",
    "\n",
    "For each char in the input array, the target array give the char that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs [25, 49, 38, 2, 46, 48, 53, 49, 42, 49, 40, 8, 2, 56, 43, 38, 49, 2, 18, 53, 38, 40, 48, 53, 2]\n",
      "targets [49, 38, 2, 46, 48, 53, 49, 42, 49, 40, 8, 2, 56, 43, 38, 49, 2, 18, 53, 38, 40, 48, 53, 2, 29]\n"
     ]
    }
   ],
   "source": [
    "p=0  \n",
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "print \"inputs\", inputs\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "print \"targets\", targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad to update the parameters\n",
    "\n",
    "The easiest technics to update the parmeters of the model is this:\n",
    "\n",
    "```python\n",
    "param += dparam * step_size\n",
    "```\n",
    "Adagrad is a more efficient technique where the step_size are getting smaller during the training.\n",
    "\n",
    "It use a memory variable that grow over time:\n",
    "```python\n",
    "mem += dparam * dparam\n",
    "```\n",
    "and use it to calculate the step_size:\n",
    "```python\n",
    "step_size = 1./np.sqrt(mem + 1e-8)\n",
    "```\n",
    "In short:\n",
    "```python\n",
    "mem += dparam * dparam\n",
    "param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update \n",
    "```\n",
    "\n",
    "### Smooth_loss\n",
    "\n",
    "Smooth_loss doesn't play any role in the training.\n",
    "It is just a low pass filtered version of the loss:\n",
    "```python\n",
    "smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "```\n",
    "\n",
    "It is a way to average the loss on over the last iterations to better track the progress\n",
    "\n",
    "\n",
    "### So finally\n",
    "Here the code of the main loop that does both trainning and generating text from times to times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 102.771844\n",
      "----\n",
      " iyJYy:\n",
      "jlO.h  yANlAHYm\"wmr?GbF S:VpJga,pA:mzJOEY\n",
      "NhVho JWze.jEVI:?kFejuxn!\"whmQ-Ys?zT,zJ\"cF\n",
      "Wza\n",
      "MmbO,Sn\n",
      "Fky(SBcIODu) ruYkEzqQdTGEVETE(x'TLIuFSdMzxPzDsgoefynCQQY?mEa)mJd,skWLgkh\n",
      "vM NYg\n",
      "sYut ye-PETbmaE- \n",
      "----\n",
      "iter 1000, loss: 83.755345\n",
      "----\n",
      " an one, wero sesu r. we \n",
      "alu nese caghe ghein be hhamltll inn ta\n",
      "goarrus ane  t s fo aik. ef bhe wne navpe e gwethe nte fcp heson;  on eotk swe thh t  cog hug reh thut tho hatos  wal mheen anr  a?ny s \n",
      "----\n",
      "iter 2000, loss: 68.699145\n",
      "----\n",
      " \n",
      "yo No holveabat ot., morhtop\n",
      "thi, anreawastathiy hor the wor foghanghas\n",
      "pler \n",
      "woy. oord whot thousgoily at loranghe shat rork angeveope ne pistann than toshevuod hooncasshat nathond thoAnge af tita h \n",
      "----\n",
      "iter 3000, loss: 60.579479\n",
      "----\n",
      " oals SOrreybit womis awewiveeont oxllnet dory ham coser nos doateyeveawaod and\n",
      "oois hud. ot tallyiclisd thas\n",
      "wid't af\n",
      "mowiwoy somedpiswor souln,\n",
      "to could fobapcere wamsow woly wiclyid wasinoven in!srt \n",
      "----\n",
      "iter 4000, loss: 56.301380\n",
      "----\n",
      " e fyite, to the themin thro hin bhege mived\n",
      "and brer Grin tinger the thad\n",
      "rult abd Gr,\n",
      "theryrle be to bendo ftoug s of glrausp the of fut in .ust\n",
      "enm, las. at wutisoo of the were the t, to thot tere\n",
      "w \n",
      "----\n",
      "iter 5000, loss: 55.971718\n",
      "----\n",
      " The war and corely woce toit co hick waster hable vong ki.  aolepy Greveutrory\n",
      "sount de mougpe.  Gregoly\n",
      "assald utmed be.\n",
      " t n- ingm wed  invismobe; and wompion\n",
      "pust, s hes  drove\n",
      "keould of ast, it ne \n",
      "----\n",
      "iter 6000, loss: 54.544737\n",
      "----\n",
      " le chas ssa ly in, bred thesi winf dow morts andrartas, his lt they'f whicryoveg ark.\n",
      " A\n",
      "d kearald seely ofred the himsey byimt nest the him\n",
      "tor\n",
      "the  to  At\n",
      "thas brery duvirgs\n",
      "thy\n",
      "sheit inga peyt atly \n",
      "----\n",
      "iter 7000, loss: 51.850908\n",
      "----\n",
      " wougorch het to letstrea ing pull he nald to but\n",
      "evew wlod, he threaslate Gregor her hin all he the walls if lo Gregor's leke.  Wuses susethe bey do to ghethald his tine in warlen the do whe intwofre  \n",
      "----\n",
      "iter 8000, loss: 50.585152\n",
      "----\n",
      " has uner sI\n",
      "ffou vo coule; to loon trill mow moif couger\n",
      "ald wother uny\n",
      "soon had simas in fuly and him exmerend (ong't thend her peacl y. pAsthe undis sale wirhibut his soo- - ovens, qengen cangings n \n",
      "----\n",
      "iter 9000, loss: 49.793019\n",
      "----\n",
      " her gove to faiving shag Gardilvir s her, asen.  Bavifn.\n",
      "\n",
      "uttill, ant Igen could, ont ou and gower the to eniti the cler iom hal reat\n",
      "ars him.\n",
      "\"ut fise\n",
      "on to wich. sery to the not\n",
      "soun out agar ts, fu \n",
      "----\n",
      "iter 10000, loss: 50.262314\n",
      "----\n",
      " ; befs\n",
      "and he aly bet thang ever shougst ady an lich.  Burd was he thablaifdy bais bid doming not mocs uskew.  Acee in the of\n",
      "overyit wat\n",
      "fels, from goin lrymuvearend bungor the to herary of he of in  \n",
      "----\n",
      "iter 11000, loss: 49.552466\n",
      "----\n",
      " in rimingst and fowhouikfughen hinge if was thangen the faised the wast il,nater some nete and woure sisher all ting andundsing the!n thebind, clyecineinis to his hom this his sout the bat hisnint the \n",
      "----\n",
      "iter 12000, loss: 47.902332\n",
      "----\n",
      " seif betifven and toor lither ppiding, sois, it him wasteimpieg intleden Gretha keerentt woulopellmed\n",
      "fornen bugnellla be wirly cagr of at thely then\n",
      "qurery on hes neey of no and as hoult the wirs tha \n",
      "----\n",
      "iter 13000, loss: 47.453582\n",
      "----\n",
      " er welereaby\n",
      "the way sed fiot thar clollind nof of\n",
      "- aprend aittain blat at had the seey but boon, af hor wabtadn as anot rith and has hiod of the war the the coun af hir.\n",
      ";\n",
      "bthe to way fore, onkimely \n",
      "----\n",
      "iter 14000, loss: 47.200488\n",
      "----\n",
      " \n",
      "vrorn beving the shatad neving in time that steath tbom, watk, putdow the couldd nouce fads shis fon.  Sayid his bess to eaby cous lidsel them, Gablew of oret.  But not.\" \"Ock. \n",
      "Share shy the hay, ot \n",
      "----\n",
      "iter 15000, loss: 47.764175\n",
      "----\n",
      " neen bet acha the the's buis;\n",
      "sist he sBell wher. S\"\n",
      "he aning st har\n",
      "fuven He vors the gaze whick all\n",
      "boverma ow st\n",
      "mok the al he and camen that becar nape wat abalk.\n",
      " Bnt forping, betion stulmst it.  \n",
      "----\n",
      "iter 16000, loss: 46.735287\n",
      "----\n",
      "  thas head lopsingo sise of he , othting the\n",
      "easnew, nove thim atly riked wher iover dind\n",
      "-\n",
      "antupsimpr;oug as woven, bemed iata he sare feemithor.\n",
      "\n",
      "af he Gregor's mofine he sadlr,\n",
      "gocing oHe liveun fi \n",
      "----\n",
      "iter 17000, loss: 45.711782\n",
      "----\n",
      " ind had Gregor at\n",
      "re sither the ded sast in thee as iori whey he wad would fors,\n",
      "fur how it drobof. \n",
      "Gregor coughilst his , thet in them the whilps thoughing (oalrotted sood at hes y, wess mode he's l \n",
      "----\n",
      "iter 18000, loss: 45.301988\n",
      "----\n",
      " to eakrind groof whrout nor and byd\"wheen not eivime coln.\n",
      "thim as in the work of for\n",
      "the lef.  Fot form el\n",
      "\"oom. \n",
      "They\n",
      "to un turm, sind as astey werills sbar\n",
      "wveren tinled, where in sle taden.\n",
      " The u \n",
      "----\n",
      "iter 19000, loss: 45.844713\n",
      "----\n",
      " or, even atlead\n",
      "of in, and that mathed doum \"Ma!\" lo from the ward, out in vot alllying dad sumy\n",
      "mother, a be shew. .\" dead diful \"itlet his poathing out ifwass t om.  Nome his werker mling rifuad he\" \n",
      "----\n",
      "iter 20000, loss: 46.679648\n",
      "----\n",
      " hener\n",
      "that the endeckink stalln wele\"m asoughing tre in his ure leeffich ofr't yom mseyser hastn Gregor she whicet coll thay mocasd, and rot with\n",
      "keike no op not eagh beeny houn Gregor's sash!cancwist \n",
      "----\n",
      "iter 21000, loss: 45.248062\n",
      "----\n",
      " nt, the\n",
      "had buthe  got if his matha the gor. \n",
      "For they he out he , Grigoun him seifed coul. -\n",
      "a that btome theurd, aboully at dother tomleay findorted had atile this then the fioblished saring all to  \n",
      "----\n",
      "iter 22000, loss: 44.345717\n",
      "----\n",
      " o the god not reamald a the tforit or with of butuy to tire junted  Greged on hat wherfed furnithing tle sas to not over Inher, he't wall and ant obe diening to sert; and wher he routely couscione the \n",
      "----\n",
      "iter 23000, loss: 44.199200\n",
      "----\n",
      " d ouce\n",
      "wat hean that t ourn I make taped onthing tlermely with that pather that he placle Greter was wist h and the efelfiuth me, oun; beafumase in tham ha- to to to everyt her thy peet beesm plire\n",
      "th \n",
      "----\n",
      "iter 24000, loss: 45.388264\n",
      "----\n",
      " ats, lame broutun wethess!\"D appunbomed wormoboaly fithe room had ruven't hat blout; feot bet of evenfjlimel\n",
      "out urder thery cance cat;\n",
      "If he bet in pexture,\n",
      "he hadmed ag to\n",
      "If s in to he siator his i \n",
      "----\n",
      "iter 25000, loss: 45.355401\n",
      "----\n",
      " Med, ough, Gregor mut out one on. \n",
      "He the flit ret?\" I woull whing the came and m\", as cleidry t agiuth - hatwout be anciuld the canh merkally\n",
      "and tarent upperen ould this.  Wersever at as movened sim \n",
      "----\n",
      "iter 26000, loss: 44.150509\n",
      "----\n",
      " ounply a whuncau,\n",
      "bus do muthen\n",
      "siin an babmest cliige evenly she wofly theed thend\n",
      "horr as through, herreasition they her aiting to themed the wastey conefle soun fadentaoflither and just nepsally mo \n",
      "----\n",
      "iter 27000, loss: 43.710765\n",
      "----\n",
      " w one ave uven him out, fyom he openiblough habked f other, lo\n",
      "pmingore st ort notemoversted.  No not work, from pas antoifsey, it on\n",
      "the foot; and the room and ar\n",
      "thime fle strront on his becai\n",
      "thin  \n",
      "----\n",
      "iter 28000, loss: 43.461578\n",
      "----\n",
      " iole\n",
      "Gregor'l and had whitelp It.  Greger, the she wame heher tho git.  He, the ondote, taps thid -, of hut\n",
      "\"He all lee coumong him a dingly on wothtor she at hom towincemen her as had, hom bare\n",
      "now d \n",
      "----\n",
      "iter 29000, loss: 44.557924\n",
      "----\n",
      " eds wether\n",
      "ree founded for hem to chrew hoated\n",
      "at he leatring red.  It he tunsed bmong dreaminsull gerst ly, at.\n",
      "wellin  forlo that and futher jakelw ould becom ne had oncont he pangeave not pricu wer \n",
      "----\n",
      "iter 30000, loss: 44.259704\n",
      "----\n",
      "  ffock in find had and he dimpat siftel, Gregor saduth so her yot for him chourd to hadpinstullfor!, dan imswer, be all in\n",
      "centroftay his frepen\n",
      "lociong, clent and figsist faen fleked alt to not.  He' \n",
      "----\n",
      "iter 31000, loss: 43.084238\n",
      "----\n",
      " iod the wertor, for the his fatithand hef.  On'tfelf ad to stilate.  Onrey to doowingreleshend, then the lould for him\n",
      "she everythen as bethainy haved igh\n",
      "some, en wherenter the barmoded in to the dar \n",
      "----\n",
      "iter 32000, loss: 43.020588\n",
      "----\n",
      " slift him\n",
      "he hams\n",
      "in t Gregor'g, where ot; at on thit?, sit his got time the the dart\n",
      "for thouly ord cam to the,\n",
      "Gregor; leepen path the sain wan salk then to er\n",
      "tore bet, herr tour a do been weke the \n",
      "----\n",
      "iter 33000, loss: 42.941185\n",
      "----\n",
      " ontresuld melt, the len intertating that to Gregor his seen.  He widlint\n",
      "kast stel the falc.  Eveny leated he not had stronnibling if but mone ang.\n",
      "\n",
      "It his\n",
      "or \"antispeatad fropruyed forner woored unda \n",
      "----\n",
      "iter 34000, loss: 43.656549\n",
      "----\n",
      " aidm, lees.  But in thingoy thin pliate sst he his.  But ent\n",
      "about lok, hasts eveny not fabllsmable a lile ppote sorever.\"  Mr hat.\n",
      " \"Yeneepdond, and the was one llonk want even ratuy besune premen ib \n",
      "----\n",
      "iter 35000, loss: 43.176001\n",
      "----\n",
      " ce from s, eMf plegstough id atto erd a was weleald ara the fatien of wiulf doom, sod fetles attor could argerel.  Now wat her theed put ievindling in I dealees collowas have unou doweding fanted and  \n",
      "----\n",
      "iter 36000, loss: 42.285351\n",
      "----\n",
      " sten ly\n",
      "that, by lather\n",
      "himkiot for to en'sesi sharjlroming about came not upen the furnion't in't to forcout on\n",
      "thoik have\n",
      "to dounat not lumg, him they thone it othily deck it than mother, Gregor\n",
      "him \n",
      "----\n",
      "iter 37000, loss: 42.085799\n",
      "----\n",
      " se whence as she ofred in the en mut bask she rad had theely breare\n",
      "ts thad the sire itturw feaus pletered mate; beedening the chaid had stionged statninged mitter ctore in the\n",
      "fornisten in of there s \n",
      "----\n",
      "iter 38000, loss: 42.568645\n",
      "----\n",
      "  on\n",
      "was these whatior alts'st no then the opending three where she awneary the warkind and beren.\n",
      "\n",
      "Dom prever thim. Sou cotlernep\n",
      "tleethen medy bo breasery, mochoming pars up,\n",
      "they\n",
      "to for and comst th \n",
      "----\n",
      "iter 39000, loss: 43.550616\n",
      "----\n",
      "  it would\n",
      "lo elleted\n",
      "slmed antoun of conce wether bramusk nemad he was I rant of could\n",
      "could for peable srybe vups of the had taontanjy the firriasing the a be thing plitcteaked he thay he furst to un \n",
      "----\n",
      "iter 40000, loss: 42.599553\n",
      "----\n",
      "  thet not\n",
      "in the moring now deertion toig thlist.  It ot see notrise.\n",
      "\n",
      "With growiseh aow, went\n",
      "in the krow air they a that It coughable about too in would he\n",
      "re\n",
      "back his or he was\n",
      "soworke alad.  Thon  \n",
      "----\n",
      "iter 41000, loss: 41.706770\n",
      "----\n",
      " ld No livest order, hat was st a rooping harly oprikis pooke next had and gookels. Sou trout it wands ig\n",
      "to s\n",
      "rood wortared in there all to kelet,, brout, of as has in she tour ank hall thesk not and  \n",
      "----\n",
      "iter 42000, loss: 41.601795\n",
      "----\n",
      " ilet there elearlen trered enthour, morge teans the clobticing\n",
      "on his still his peen piivten stant ofts he seening in the fortor the was\n",
      "overy very wnowtwhiss in it him mand see reazt to\n",
      "any and it ar \n",
      "----\n",
      "iter 43000, loss: 42.590356\n",
      "----\n",
      " to as foud\n",
      "\"Jood pooke tlard bect alsess slich he kell, beed begnuch, bent, betared.  He -\n",
      "and sis iel his round he\n",
      "her hus hid geves soug houhed it the doiletuient all out intas, the was wislee\n",
      "lered \n",
      "----\n",
      "iter 44000, loss: 42.815417\n",
      "----\n",
      " n of it his he caring the all to doo spate frow\n",
      "defance no that for his being than elly.  For gut t, courd exthes the elrowed be not grteing to the butha the.\n",
      ". \"What's their of his pleno-tulen rowsis \n",
      "----\n",
      "iter 45000, loss: 42.019878\n",
      "----\n",
      " tionicrlock, the evence his fother, the fingit ext to the foretiond closs un the\n",
      "knowed, to heavivinn the sire breof he room worring of nown to to row on the wamlobleted and rimert was these tho flowc \n",
      "----\n",
      "iter 46000, loss: 41.480321\n",
      "----\n",
      " alraster the open congurling in it. \n",
      "He talint the\n",
      "gove ciand ing\n",
      "the door hibfuseternep withor that heleal harghee his painter botionpo back adis ipped pire himst ould now his was by f or ever on of  \n",
      "----\n",
      "iter 47000, loss: 41.325652\n",
      "----\n",
      " efele him room furshersed mother on the krace, armat underes.  Head that susbedsacts at pousher wocle, as if it had knyen - instive there itself, firnand a-ter's fexthen of hrit he winyougst to to whi \n",
      "----\n",
      "iter 48000, loss: 42.597408\n",
      "----\n",
      " ine nonst lapence after it\n",
      "on would to she bock Ffousill bet how vis ffost ans came, it oc\n",
      "only and door himsele.  The was slat hassion\n",
      "flealy\n",
      "quite each to stand hell asly for gotand in he sa whole t \n",
      "----\n",
      "iter 49000, loss: 42.437218\n",
      "----\n",
      " bething of his stered for to th, the dra. wer his\n",
      "ther thay.  He hissing and notr chite I\n",
      "Your sey uprees.\n",
      "\"At\n",
      "forter, but in his spat other, Greger.  The to hess fhew mous and this dut I'le slolrtale \n",
      "----\n",
      "iter 50000, loss: 41.420594\n",
      "----\n",
      " e. barlich it havisgeen rhey nowh Gregor's nee from with whied weshans slisly been us was to the furst was sistrring iomisedualen Mot vore hourd om no\n",
      "more urewny, it was whiod than and of hem.  Grego \n",
      "----\n",
      "iter 51000, loss: 41.272323\n",
      "----\n",
      " t her havis was he was every dowinly sis they to to ave\n",
      "jad anefonged for with soliven, and been but\n",
      "to as pestorg.  Bed\n",
      "and till weplet's hoin thonetundeds but in the leatilm, sisten to sityouth the  \n",
      "----\n",
      "iter 52000, loss: 41.120956\n",
      "----\n",
      " sed thrork but nothing and deying his room. s\n",
      "havel.  His and her whion and them and it hopr at the chourdiatha and on the door was clieg thetr more?\" had sustings carcoussing didingsble of to uge igh \n",
      "----\n",
      "iter 53000, loss: 41.931471\n",
      "----\n",
      " \n",
      "that his\n",
      "furlice the cilask at pats it him muchilt was the faiver vurce, kind deer-ed hut lay meclocked his sailly ou shing look leed he had salt he gom whomethre underster opmeter in thak it.  And G \n",
      "----\n",
      "iter 54000, loss: 41.575497\n",
      "----\n",
      " omeh her tien shopsilily gehen whith while alirten preard Gregor wastly would moven for agamad on of sood\n",
      "had ar have entire at over at rown'f not that him eaching did geed to mad wadk, wherwey airlai \n",
      "----\n",
      "iter 55000, loss: 40.715546\n",
      "----\n",
      " hing was severning first her tirter as have that's behershisseet\n",
      "fill had aboundcaing, as hew ioft the door how\n",
      "baen of appime and died coming footly into be wey the wagted himfed yster\n",
      "becume tinter  \n",
      "----\n",
      "iter 56000, loss: 40.725483\n",
      "----\n",
      "  awnon, till, from the cholr, but he sheen his drore thing and havlowe was nove alled horr able fon troon be not thear. , usrerure ever\n",
      "on ent hed, and there or had astent and he had congerpsed shinge \n",
      "----\n",
      "iter 57000, loss: 40.997292\n",
      "----\n",
      " des.\n",
      "\n",
      "So brounddirned sad best lupsed he didr a lwougha!\" wat her eack medd atsa to home in their conlingsa\n",
      "into.\"\n",
      "go\n",
      "lakbeqly\n",
      "updintare the.\n",
      " This bristed and beaned\n",
      "be navilw\n",
      "mother ainirtere. , tla \n",
      "----\n",
      "iter 58000, loss: 41.905852\n",
      "----\n",
      " at schen hossileverly this agaiss.  Gregor saytrent andong\n",
      "would\n",
      "acked as tomed\n",
      "came. \"Scally jugning, and but he walean thro and I'm themened and suin, work his foreering, onch to can a cooling sised \n",
      "----\n",
      "iter 59000, loss: 41.149054\n",
      "----\n",
      " ne onediad this bain enthreared\n",
      "or coverinely goc,, new fle ress as Gregor lookel.  Qer pingle sook in ured into turned -wall trout on the with has sest fust even he loo he was and, the shat was bethi \n",
      "----\n",
      "iter 60000, loss: 40.277735\n",
      "----\n",
      " ee was\n",
      "sossace arse and and uflets.  The cod.  He doirreat hiccery nownruse coobee her\n",
      "proso\n",
      "to by as futt to mokenghing while loor, to brome to ann he read surs as alst werlreffore that him\n",
      "en no my  \n",
      "----\n",
      "iter 61000, loss: 40.267398\n",
      "----\n",
      " ead sest that hurin and and.  Gregor whenly the though, alled a force intmeat wuscly firloyed on thew deavicesiciois, sNowly to the ellent to ank it tly altite thetan's could; again, they sunt.  That  \n",
      "----\n",
      "iter 62000, loss: 41.045037\n",
      "----\n",
      " wers thay ren that ans every ioum and hat ease the trereder take thimstely bugg ond paited them's was as drownel ar bictha mout har allilarle their was he thrievilvn a lice he camited alled tule save  \n",
      "----\n",
      "iter 63000, loss: 41.499413\n",
      "----\n",
      " reatious net sed the kand the even not eed that he rerited the turribly that ius\n",
      "exaldneve op fey the been hit\n",
      "seeclewed bed\n",
      "bust sfren this for; ext to he lime, a!\"\n",
      "\n",
      "It eaprits the gen the kirs as sh \n",
      "----\n",
      "iter 64000, loss: 40.726280\n",
      "----\n",
      " , do aboutling and planom the cour out so eres\n",
      "somates to have clerly pithen. \"Lent fiod - podeobe wond he spand then the moxed,\n",
      "showe frome thetr could on her of no bect. Sams of ge sirs a bict meabl \n",
      "----\n",
      "iter 65000, loss: 40.215969\n",
      "----\n",
      " wrelly this as boway,  sluses to the moddiblise; beh, govend to againponing hirdent and insey\n",
      "as his mowenge lost mith to not somtind pain. Gregor courss I'm good, any as; to ho rous tire trmal then l \n",
      "----\n",
      "iter 66000, loss: 40.031820\n",
      "----\n",
      " ss with had was mexiagly st heply the geetelly to all, to vertinnint, whill\n",
      "that and dost alsite, doned\n",
      "hisid tuove, wimerlrisind was grethapprind Greabll had\n",
      "with the\n",
      "plough, just but nothres and tir \n",
      "----\n",
      "iter 67000, loss: 41.296898\n",
      "----\n",
      " n\n",
      "to sting thet\n",
      "werered\n",
      "scatel chout in ill\n",
      "pleak\"I', clere a faecelfous an) had pat curtured. ,\n",
      "\"ave\n",
      "have toongs whit anytion the cosen and, pligsises -'s back s-aints aint.  But aovile a lips was lo \n",
      "----\n",
      "iter 68000, loss: 41.331681\n",
      "----\n",
      " byer tood Gregor wosforter, to for his hand caves she were other sasger these for I'm in it.  Thy of hosfarsed thone whongerfey to Gregor will their was himself at himplyffich the\n",
      "for agarled to wheng \n",
      "----\n",
      "iter 69000, loss: 40.500553\n",
      "----\n",
      " od\n",
      "int leps tive for\n",
      "at the\n",
      "erill! and he, the goo cough it his\n",
      "sister his\n",
      "- the saine sster, aly get more them that no thaughist himself\n",
      "sough out the chaike thior he reawither you could he was it te \n",
      "----\n",
      "iter 70000, loss: 40.086557\n",
      "----\n",
      " t where peege it.  Wenter hope the tadeed the epplove the\n",
      "courss he his slamube would hally into bat in wouthep ar him and it whenel\n",
      "been deared\n",
      "he hand whild but ickly\n",
      "difrot would as pare st condere \n",
      "----\n",
      "iter 71000, loss: 40.095164\n",
      "----\n",
      " taning\n",
      "a the thay hay - the tlat\n",
      "they doin to clore, whanither's aonint\n",
      "room to cheer \"Herout in the cive\n",
      "time.  His had aly it was a wered to at ope ibolt, otherly fared in his st his caming the wagi \n",
      "----\n",
      "iter 72000, loss: 40.815433\n",
      "----\n",
      " oware, sun Chact and and for even ncrulted it the fighth and sta whuulr\n",
      "he could, poshal of his fett\n",
      "was become been fam, abmo food\n",
      "was pothen. \n",
      "At do was he pwalp\n",
      "long it was - fap his sforn dint und \n",
      "----\n",
      "iter 73000, loss: 40.492776\n",
      "----\n",
      " s parle now of his recay for into that the uliss veronillf at his rooms buinedfinvistible.  Mrsyermemped to nothainngs to move again,\n",
      "come and haveved new conseved trould alant werted to ta, being wer \n",
      "----\n",
      "iter 74000, loss: 39.858334\n",
      "----\n",
      " r and disticencark than sore had more the gen the door ly so the eakemove it wert room and\n",
      "coute say?verese with to sitcled to her able\n",
      "into drawceming outimy the ganthew upfochind onit she had have s \n",
      "----\n",
      "iter 75000, loss: 39.808056\n",
      "----\n",
      " ger, and hadp turs of his stardednesstanises ento\n",
      "emal to would and paed,;, over to quiuld himnent!\"  Ac thong ofwed to she sair darants, and than must\n",
      "keycemuld, ontroatile yest beftat anrer!\"L\n",
      "yGreg \n",
      "----\n",
      "iter 76000, loss: 39.792912\n",
      "----\n",
      " tirter, aonis; this wayfen from the stappletr look it, caked\n",
      "with ghith aboustlith backarem.\n",
      "\n",
      "\"Cugh!\" had before evere get\n",
      "dowate of meed it room in.\n",
      " It fhat now fhem.  But out wuire theo gropk.\n",
      "\"Cea \n",
      "----\n",
      "iter 77000, loss: 40.582376\n",
      "----\n",
      " r, and for.\n",
      "'s the clief and she say roor be\n",
      "dire.  You had prown now nowine.\n",
      " Arminds and yes, to away to sasing any have expecked live us\n",
      "Gregor, lay I'm seid bevurised his dery had lay Gregor onfo  \n",
      "----\n",
      "iter 78000, loss: 40.007074\n",
      "----\n",
      " , jock\n",
      "when her spaitioving in the door aroun and docious and nometpong but his knith op\n",
      "not seat iverething his twopcers un at moun\n",
      "mught ofred in sely\n",
      "a lecker to on the door\n",
      "mother time, ought to t \n",
      "----\n",
      "iter 79000, loss: 39.370803\n",
      "----\n",
      " o she be that that the from the sidy deevere\n",
      "oul to steas, a fuincent her ap-ely of the of his whating, slad\n",
      "pawent, at lot, then fars.\"\n",
      "Garted\n",
      "ownile adsiltre Gregor teme to clant he geest could clee \n",
      "----\n",
      "iter 80000, loss: 39.281780\n",
      "----\n",
      " paking\n",
      "with siice, allen themely om the sike the fiemold whjed seed dount, cutce left of his thenely taken and the vight she women the\n",
      "m wougermed back had have it even uneated in misting their was .  \n",
      "----\n",
      "iter 81000, loss: 39.824211\n",
      "----\n",
      " .  \"Just each call\n",
      "quiech had they his peate.  The'? he was\n",
      "plashing lood the living to befayd\n",
      "on they\n",
      "briably jut them worring.  The had were her not she saying were was to the foom with\n",
      "taken rome f \n",
      "----\n",
      "iter 82000, loss: 40.644151\n",
      "----\n",
      " ould the frols steven the one cut with the pressas, as and tnolen longy than that to not in the kind he was the gentlet whill cought open procs I wene they rempaly up pain, their doondly he\n",
      "allawnsten \n",
      "----\n",
      "iter 83000, loss: 39.815389\n",
      "----\n",
      " t he was bequred. 'Wsey, soude mafter calm everytion time not be and had back\n",
      "of he was home of\n",
      "so as any to en\n",
      "Mry want.  Therethonced, st - the k as. Sour.\n",
      "\"Becet a lombechare of her was as in well  \n",
      "----\n",
      "iter 84000, loss: 38.979579\n",
      "----\n",
      " r did bething feys sight anledayge lap, the says fake in his beassied\n",
      "treadely he bacd him if serhawnew ress.  Nom.  Where! Som wime was had\n",
      "everys; allat somenge wouldr even neter, had, were used it; \n",
      "----\n",
      "iter 85000, loss: 39.159863\n",
      "----\n",
      " fromingened (yin she backiel oply\n",
      "bistiss meen him onnt, purcalling of\n",
      "his taly be slock tolr\n",
      "all thriw then, say.  The midy thourly intersy from fere; ic towcorsbower aribarn.  Un wime begencal.  Oft \n",
      "----\n",
      "iter 86000, loss: 40.319142\n",
      "----\n",
      " her on a looked in all cituate, novite. ,\n",
      "have there, nod was was hear by mor! He had whicr, he about ints\n",
      "bain to doungning shising long atneates.  The difill s was sisthaon's reasponis he handly wit \n",
      "----\n",
      "iter 87000, loss: 40.557486\n",
      "----\n",
      " itull, sight was that he eangly to get.\n",
      " Though that the cheok and forminjust hlove about agare\n",
      "I kniffodis\n",
      "would away the dooms as nother alst\n",
      "out bely pook, would st as his mother? surced as threa t \n",
      "----\n",
      "iter 88000, loss: 39.526459\n",
      "----\n",
      " o the ans as by through from his ding body brist\n",
      "the choul\n",
      "in that Gregor's gnat in of no vow. ,\n",
      "Gregor come mit room his s find there, shoughmon to, we ore coull that him\n",
      "lo joment of her cpuady she  \n",
      "----\n",
      "iter 89000, loss: 39.232618\n",
      "----\n",
      " traning; fone or did that horrt, aid of the dlaw long himself\n",
      "cut of wowh othen so let about might own dreasilus into it toun\n",
      "lo do on his could thenger and was\n",
      "worrous thet own't bewor\n",
      "tlood. \n",
      "Hem do \n",
      "----\n",
      "iter 90000, loss: 39.126086\n",
      "----\n",
      " our\"Grcthing st\n",
      "he hand to mister thry haffing the would becume sime.  It reave id, with it\n",
      "of if the liscom the only limsarsylister\n",
      "alatioc.  Suts pare to butt ot't thrich was, I've this sade of the  \n",
      "----\n",
      "iter 91000, loss: 40.047469\n",
      "----\n",
      " \n",
      "\n",
      "But ffen out forgely mother and sting himiled in,\n",
      "he wast: \"Ahad too the would dewaway was vo expeemay, as wist of airing of coups to tise herm them and himmel been the gant forteny agire gendlateve \n",
      "----\n",
      "iter 92000, loss: 39.818793\n",
      "----\n",
      "  him.  \"I he earmost to.r wookt now of it was any hadp I'd onthit\n",
      "the door had as one that he way had have plegull\n",
      "Gregor stard.  He could of he pachisted, the falt.  Pen onto mead theply now, perhare \n",
      "----\n",
      "iter 93000, loss: 39.102277\n",
      "----\n",
      "  of, shance\n",
      "as to her bernoffist, even then them again heally to be in hibked hore now their\n",
      "rropart of his un morning one on ithive and it had about to seem, able her.  For but and before tlaced when \n",
      "----\n",
      "iter 94000, loss: 39.070557\n",
      "----\n",
      " as shought she harded was was thew dy bun to wether har alw\n",
      "helet had had sted\n",
      "forwouse sight omst in\n",
      "he wankers,\n",
      "his - been throp's the deacce, aldate of the thau\n",
      "htinguldnother, him Gregorch appeace \n",
      "----\n",
      "iter 95000, loss: 38.942291\n",
      "----\n",
      " caurd s could unly\n",
      "dooms he three was had semed her loget not fince lettle futhing of iftend as a lather.  With hurring out she stort his so get well bet\n",
      "feen new she ally lay bestiny out the epening, \n",
      "----\n",
      "iter 96000, loss: 39.789890\n",
      "----\n",
      " usen the donning it hopr the els, to mother \"Buthing say.\n",
      " I\n",
      "\"Heteed to threas to doirly\n",
      "chich far on and a ko deif youre\n",
      "caunedy\n",
      "he's not fored it, beem, sple she fre wind\n",
      "the sasich it hishings, his \n",
      "----\n",
      "iter 97000, loss: 39.221689\n",
      "----\n",
      "  have\n",
      "the wime.  Nont he gevere unsed that his mother noch finulints proce\n",
      "that he durd\n",
      "ention wente to at even if the doorwaye allway, we could only lilf in these when! She ded had the door to though \n",
      "----\n",
      "iter 98000, loss: 38.789771\n",
      "----\n",
      " would now do no thrier surply who was core and more ffour of it would osject\n",
      "of it purss it was room.\n",
      ": ghalcterter, and trough, at\n",
      "when of an over thing came to whomesped\n",
      "for would some than had\n",
      "deat \n",
      "----\n",
      "iter 99000, loss: 38.648902\n",
      "----\n",
      " t of\n",
      "harly to musned.\n",
      "Wtalood tuching out; wosting to than, no air and open would\n",
      "had the was hurre was in urd in its blay, the cam heryed stur turnes, all and his fatke only lad the droa regs\n",
      "could n \n",
      "----\n",
      "iter 100000, loss: 38.958929\n",
      "----\n",
      " ly bed uswerces. Som, they\n",
      "lecementsing to the mpreem to solaterwed, and she evend of the with p. bicg.\n",
      "\", on coly lack but to his lost was overSore, there in any, amincers come somesint she wlokes th \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0                                                                                                                        \n",
    "while n<=1000*100:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  # check \"How to feed the loss function to see how this part works\n",
    "  if p+seq_length+1 >= len(data) or n == 0:\n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
    "    p = 0 # go from start of data                                                                                                                                                             \n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "  # sample from the model now and then                                                                                                                                                        \n",
    "  if n % 1000 == 0:\n",
    "    print 'iter %d, loss: %f' % (n, smooth_loss) # print progress\n",
    "    sample(hprev, inputs[0], 200)\n",
    "\n",
    "  # perform parameter update with Adagrad                                                                                                                                                     \n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "  p += seq_length # move data pointer                                                                                                                                                         \n",
    "  n += 1 # iteration counter            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedback welcome __@dh7net__!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
